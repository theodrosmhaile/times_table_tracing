# TafelTrainer Data Analysis

```{r setup, include=FALSE}
# Load required libraries
library(skimr)
library(tidyverse)
library(lubridate)
library(anytime)
```

## Description

This analysis prepares data from Levels 1, 2, and 3 of the TafelTrainer dataset. The goal is to create datasets for GraafTel input (first, middle, and last encounters) and conduct descriptive analyses.

## Data Preparation

```{r}
# Load dataset
dat <- readRDS('./TT_data/tt_responses_all_clean.rds')

# Initial filtering: Ensure 'correct' is numeric and clean `cue_text`
dat <- dat %>%
  mutate(
    correct = as.numeric(correct),
    cue_text = str_replace_all(cue_text, "\\+", " ") # Replace "+" with a space
  )

# Define valid responses (numeric values â‰¤ 100)
valid_responses <- 0:100

# Filter for valid 'given_response'
dat <- dat %>%
  filter(as.numeric(given_response) %in% valid_responses)

# Add multiplier and multiplicand columns globally to the dataset
dat <- dat %>%
  mutate(
    multiplier = as.numeric(str_extract(cue_text, "^\\d+")), # Extract the first number in `cue_text`
    multiplicand = as.numeric(str_extract(cue_text, "\\d+$")) # Extract the second number in `cue_text`
  )

# Function to process each level and add encounter counter
prepare_level_data <- function(data, level_filter) {
  data %>%
    filter(level == level_filter) %>%
    arrange(user_id, cue_text, session_id) %>%
    group_by(user_id, cue_text) %>%
    mutate(encounter_num = row_number()) %>%
    ungroup()
}

# Prepare full datasets for Levels 1, 2, and 3 (including `encounter_num`)
dat_level1 <- prepare_level_data(dat, 1)
dat_level2 <- prepare_level_data(dat, 2)
dat_level3 <- prepare_level_data(dat, 3)

# Function to filter specific encounters dynamically
filter_encounter <- function(data, encounter_type) {
  data %>%
    group_by(user_id, cue_text) %>%
    filter(
      case_when(
        # First encounter
        encounter_type == "first" ~ encounter_num == 1,
        # Middle encounter: Use ceiling(n() / 2), but if it results in 1, use 2 if available
        encounter_type == "middle" ~ {
          middle_encounter <- ceiling(n() / 2)
          encounter_num == if_else(middle_encounter == 1 & n() > 1, 2L, middle_encounter)
        },
        # Last encounter
        encounter_type == "last" ~ encounter_num == n(),
        # Default: Exclude rows
        TRUE ~ FALSE
      )
    ) %>%
    ungroup()
}

# Generate encounter-specific datasets dynamically from the full datasets
dat_level1_first <- filter_encounter(dat_level1, "first")
dat_level1_middle <- filter_encounter(dat_level1, "middle")
dat_level1_last <- filter_encounter(dat_level1, "last")

dat_level2_first <- filter_encounter(dat_level2, "first")
dat_level2_middle <- filter_encounter(dat_level2, "middle")
dat_level2_last <- filter_encounter(dat_level2, "last")

dat_level3_first <- filter_encounter(dat_level3, "first")
dat_level3_middle <- filter_encounter(dat_level3, "middle")
dat_level3_last <- filter_encounter(dat_level3, "last")
```

## Export Data for GraafTel

```{r}
# Function to export datasets
export_graaftel_data <- function(data, level, encounter_type) {
  # Construct the filename
  file_name <- paste0("./graaftel_input_level", level, "_", encounter_type, ".csv")
  # Select the required columns
  data_to_export <- data %>%
    select(user_id, cue_text, correct)
  # Write data to CSV (without column headers)
  write.table(data_to_export, file_name, row.names = FALSE, col.names = FALSE, sep = ",")
}

# Export data for Level 1
export_graaftel_data(dat_level1_first, 1, "first")
export_graaftel_data(dat_level1_middle, 1, "middle")
export_graaftel_data(dat_level1_last, 1, "last")

# Export data for Level 2
export_graaftel_data(dat_level2_first, 2, "first")
export_graaftel_data(dat_level2_middle, 2, "middle")
export_graaftel_data(dat_level2_last, 2, "last")

# Export data for Level 3
export_graaftel_data(dat_level3_first, 3, "first")
export_graaftel_data(dat_level3_middle, 3, "middle")
export_graaftel_data(dat_level3_last, 3, "last")
```

## Verify Outputs

```{r}
# Prepare data for verification
# Combine all levels and encounter types to verify overall distribution
verify_data <- bind_rows(
  dat_level1 %>% mutate(level == 1),
  dat_level2 %>% mutate(level == 2),
  dat_level3 %>% mutate(level == 3)
)

# Plot histogram for the distribution of responses
# Ensures that the data used for GraafTel matches expected values
verify_data %>%
  select(given_response, cue_text, level) %>%
  filter(given_response %in% 0:100) %>%
  mutate(given_response = as.numeric(given_response)) %>%
  ggplot(aes(x = given_response, fill = factor(level))) +
  geom_histogram(binwidth = 1, position = "dodge") +
  theme_minimal() +
  labs(
    title = "Distribution of Responses Across Levels",
    x = "Given Response",
    y = "Count",
    fill = "Level"
  ) +
  theme(axis.text.x = element_text(angle = 90)) +
  scale_fill_brewer(palette = "Set3")

# Calculate mean accuracy for each fact_id and level
mean_accuracy_data <- verify_data %>%
  group_by(level, cue_text) %>% # Group by level and fact_id
  summarize(mean_accuracy = mean(correct), .groups = "drop") # Calculate mean accuracy

# Create a box plot of mean accuracies
mean_accuracy_data %>%
  ggplot(aes(x = factor(level), y = mean_accuracy, fill = factor(level))) +
  geom_boxplot(alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Distribution of Mean Accuracies by Level",
    x = "Level",
    y = "Mean Accuracy",
    fill = "Level"
  ) +
  scale_fill_brewer(palette = "Set3") +
  theme(legend.position = "none")
```

## Descriptive Statistics
```{r}
generate_descriptive_data <- function(data) {
  # Group by cue_text and compute summary statistics
  data %>%
    select(given_response, cue_text, correct, level, fact_id) %>%
    mutate(cue_text=fct_reorder(cue_text,fact_id)) %>% 
    group_by(cue_text) %>%
    summarise(mean_acc = mean(correct), n=n(), se = sd(correct)/sqrt(n)
    ) %>%
    mutate(
      multiplier = as.numeric(str_extract(cue_text, "^\\d+")), # Extract the first number in `cue_text`
      multiplicand = as.numeric(str_extract(cue_text, "\\d+$")) # Extract the second number in `cue_text`
    ) %>%
    mutate(cue_text=fct_reorder(cue_text,multiplicand))
}


# Generate descriptive data
dat4plot_level1 <- generate_descriptive_data(dat_level1)
dat4plot_level1_first <- generate_descriptive_data(dat_level1_first)
dat4plot_level1_middle <- generate_descriptive_data(dat_level1_middle)
dat4plot_level1_last <- generate_descriptive_data(dat_level1_last)
dat4plot_level2 <- generate_descriptive_data(dat_level2)
dat4plot_level2_first <- generate_descriptive_data(dat_level2_first)
dat4plot_level2_middle <- generate_descriptive_data(dat_level2_middle)
dat4plot_level2_last <- generate_descriptive_data(dat_level2_last)
dat4plot_level3 <- generate_descriptive_data(dat_level3)
dat4plot_level3_first <- generate_descriptive_data(dat_level3_first)
dat4plot_level3_middle <- generate_descriptive_data(dat_level3_middle)
dat4plot_level3_last <- generate_descriptive_data(dat_level3_last)


# Function to plot heatmaps with properly ordered and labeled axes
plot_heatmap <- function(data, title) {
  # Update multiplier to numeric for proper ordering and labeling
  data <- data %>%
    mutate(multiplier = as.numeric(gsub("col", "", multiplier)))

  data %>%
    ggplot(aes(x = as.factor(multiplier), y = as.factor(multiplicand), fill = mean_acc, label = round(mean_acc, 2))) +
    geom_tile() +
    geom_text(size = 3) + # Adjust text size for readability
    scale_fill_gradient2(
      limits = c(0.7, 1),
      midpoint = 0.85,
      low = 'blue',
      mid = "red",
      high = 'yellow',
      guide = 'colorbar',
      aesthetics = 'fill',
      breaks = seq(0.7, 1, by = 0.05) # Reduce the number of breaks for readability
    ) +
    theme_minimal() +
    labs(
      title = title,
      x = "Multiplier",
      y = "Multiplicand",
      fill = "Mean Accuracy"
    ) +
    theme(
      axis.text.x = element_text(angle = 0), # Ensure x-axis labels are horizontal
      axis.text.y = element_text(angle = 0), # Ensure y-axis labels are clear
      legend.key.height = unit(0.5, "cm") # Adjust legend key size for readability
    )
}

# Function to plot bar plots with error bars
plot_barplot <- function(data, title) {
  data %>%
    ggplot(aes(
      x = cue_text, y = mean_acc, ymax = mean_acc + se, ymin = mean_acc - se, fill = as.factor(multiplicand)
    )) +
    geom_bar(stat = 'identity') +
    geom_errorbar() +
    scale_x_discrete(
      breaks = levels(data$cue_text)[seq(1, length(levels(data$cue_text)), by = 10)] # Show every 10th label correctly
    ) +
    theme_minimal() +
    labs(
      title = title,
      x = "Cue Text (Multiplication Fact)",
      y = "Mean Accuracy",
      fill = "Multiplicand"
    ) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = 'top')
}

# Generate plots for each level and overall
# Heatmaps
heatmap_level1 <- plot_heatmap(dat4plot_level1, "Heatmap of Mean Accuracy (Level 1: Overall)")
heatmap_level1_first <- plot_heatmap(dat4plot_level1_first, "Heatmap of Mean Accuracy (Level 1: First)")
heatmap_level1_middle <- plot_heatmap(dat4plot_level1_middle, "Heatmap of Mean Accuracy (Level 1: Middle)")
heatmap_level1_last <- plot_heatmap(dat4plot_level1_last, "Heatmap of Mean Accuracy (Level 1: Last)")
heatmap_level2 <- plot_heatmap(dat4plot_level2, "Heatmap of Mean Accuracy (Level 2: Overall)")
heatmap_level2_first <- plot_heatmap(dat4plot_level2_first, "Heatmap of Mean Accuracy (Level 2: First)")
heatmap_level2_middle <- plot_heatmap(dat4plot_level2_middle, "Heatmap of Mean Accuracy (Level 2: Middle)")
heatmap_level2_last <- plot_heatmap(dat4plot_level2_last, "Heatmap of Mean Accuracy (Level 2: Last)")
heatmap_level3 <- plot_heatmap(dat4plot_level3, "Heatmap of Mean Accuracy (Level 3: Overall)")
heatmap_level3_first <- plot_heatmap(dat4plot_level3_first, "Heatmap of Mean Accuracy (Level 3: First)")
heatmap_level3_middle <- plot_heatmap(dat4plot_level3_middle, "Heatmap of Mean Accuracy (Level 3: Middle)")
heatmap_level3_last <- plot_heatmap(dat4plot_level3_last, "Heatmap of Mean Accuracy (Level 3: Last)")

# Bar Plots
barplot_level1 <- plot_barplot(dat4plot_level1, "Bar Plot of Mean Accuracy (Level 1: Overall)")
barplot_level1_first <- plot_barplot(dat4plot_level1_first, "Bar Plot of Mean Accuracy (Level 1: First)")
barplot_level1_middle <- plot_barplot(dat4plot_level1_middle, "Bar Plot of Mean Accuracy (Level 1: Middle)")
barplot_level1_last <- plot_barplot(dat4plot_level1_last, "Bar Plot of Mean Accuracy (Level 1: Last)")
barplot_level2 <- plot_barplot(dat4plot_level2, "Bar Plot of Mean Accuracy (Level 2: Overall)")
barplot_level2_first <- plot_barplot(dat4plot_level2_first, "Bar Plot of Mean Accuracy (Level 2: First)")
barplot_level2_middle <- plot_barplot(dat4plot_level2_middle, "Bar Plot of Mean Accuracy (Level 2: Middle)")
barplot_level2_last <- plot_barplot(dat4plot_level2_last, "Bar Plot of Mean Accuracy (Level 2: Last)")
barplot_level3 <- plot_barplot(dat4plot_level3, "Bar Plot of Mean Accuracy (Level 3: Overall)")
barplot_level3_first <- plot_barplot(dat4plot_level3_first, "Bar Plot of Mean Accuracy (Level 3: First)")
barplot_level3_middle <- plot_barplot(dat4plot_level3_middle, "Bar Plot of Mean Accuracy (Level 3: Middle)")
barplot_level3_last <- plot_barplot(dat4plot_level3_last, "Bar Plot of Mean Accuracy (Level 3: Last)")

# Display plots
heatmap_level1
heatmap_level1_first
heatmap_level1_middle
heatmap_level1_last
heatmap_level2
heatmap_level2_first
heatmap_level2_middle
heatmap_level2_last
heatmap_level3
heatmap_level3_first
heatmap_level3_middle
heatmap_level3_last

barplot_level1
barplot_level1_first
barplot_level1_middle
barplot_level1_last
barplot_level2
barplot_level2_first
barplot_level2_middle
barplot_level2_last
barplot_level3
barplot_level3_first
barplot_level3_middle
barplot_level3_last
```

## GraafTel Data Export Analysis
# Create mean accuracy datasets for each level
```{r}
mean_accuracy_level1 <- verify_data %>%
  filter(level == 1) %>% # Filter for Level 1 data only
  group_by(cue_text) %>% # Group by cue_text
  summarise(mean_accuracy = mean(correct), .groups = "drop") # Calculate mean accuracy

mean_accuracy_level2 <- verify_data %>%
  filter(level == 2) %>% # Filter for Level 2 data only
  group_by(cue_text) %>% # Group by cue_text
  summarise(mean_accuracy = mean(correct), .groups = "drop") # Calculate mean accuracy

mean_accuracy_level3 <- verify_data %>%
  filter(level == 3) %>% # Filter for Level 3 data only
  group_by(cue_text) %>% # Group by cue_text
  summarise(mean_accuracy = mean(correct), .groups = "drop") # Calculate mean accuracy

```
# Create mean accuracy datasets for each encounter
```{r}
# Function to calculate mean accuracy from preprocessed datasets
calculate_mean_accuracy <- function(data) {
  data %>%
    group_by(cue_text) %>% # Group by cue_text
    summarise(mean_accuracy = mean(correct), .groups = "drop") # Calculate mean accuracy
}

# Generate mean accuracy datasets for Level 1
mean_accuracy_level1_first <- calculate_mean_accuracy(dat_level1_first)
mean_accuracy_level1_middle <- calculate_mean_accuracy(dat_level1_middle)
mean_accuracy_level1_last <- calculate_mean_accuracy(dat_level1_last)

# Generate mean accuracy datasets for Level 2
mean_accuracy_level2_first <- calculate_mean_accuracy(dat_level2_first)
mean_accuracy_level2_middle <- calculate_mean_accuracy(dat_level2_middle)
mean_accuracy_level2_last <- calculate_mean_accuracy(dat_level2_last)

# Generate mean accuracy datasets for Level 3
mean_accuracy_level3_first <- calculate_mean_accuracy(dat_level3_first)
mean_accuracy_level3_middle <- calculate_mean_accuracy(dat_level3_middle)
mean_accuracy_level3_last <- calculate_mean_accuracy(dat_level3_last)
```
# Create datasets for GraafTel ratings with 3 skills
```{r}
# Function to filter and clean GraafTel ratings output
filter_graaftel_data <- function(file_path) {
  graaftel_data <- read_csv(file_path, col_names = FALSE, col_types = cols(
    X1 = col_character(),
    X2 = col_character(),
    X3 = col_double(),
    X4 = col_double(),
    X5 = col_double()
  ))

  # Rename columns for clarity
  colnames(graaftel_data) <- c("item", "cue_text", "skill1", "skill2", "skill3")

  # Filter out rows containing "student" in the first column
  graaftel_data <- graaftel_data %>%
    filter(!str_detect(item, "student")) %>%
    select(-item)

  # Remove any leading spaces in cue_text
  graaftel_data <- graaftel_data %>%
    mutate(cue_text = str_trim(cue_text, side = "both"))

  return(graaftel_data)
}

# List of file paths for all GraafTel outputs
file_paths <- list.files(
  path = "./GT_ratings/GT_ratings_3s",
  pattern = "s3l[1-3](first|middle|last).csv",
  full.names = TRUE
)

# Apply the filtering function to all files
filtered_graaftel_data <- lapply(file_paths, filter_graaftel_data)

# Name the datasets based on file names without extensions
names(filtered_graaftel_data) <- gsub(".*/|\\.csv$", "", file_paths)

# Assign cleaned datasets to individual variables for the 9 cases
s3l1first <- filtered_graaftel_data[["s3l1first"]]
s3l1middle <- filtered_graaftel_data[["s3l1middle"]]
s3ls1last <- filtered_graaftel_data[["s3l1last"]]

s3l2first <- filtered_graaftel_data[["s3l2first"]]
s3l2middle <- filtered_graaftel_data[["s3l2middle"]]
s3l2last <- filtered_graaftel_data[["s3l2last"]]

s3l3first <- filtered_graaftel_data[["s3l3first"]]
s3l3middle <- filtered_graaftel_data[["s3l3middle"]]
s3l3last <- filtered_graaftel_data[["s3l3last"]]
```
# Calculate and plot correlations between skill probabilties and mean accuracy
```{r}
# Function to calculate correlations between skill probabilities and mean accuracy
calculate_skill_correlations <- function(graaftel_data, mean_accuracy_data) {
  # Join GraafTel data with mean accuracy data on "cue_text"
  merged_data <- graaftel_data %>%
    left_join(mean_accuracy_data, by = "cue_text")
  
  # Calculate Pearson's r for each skill
  skill_correlations <- merged_data %>%
    summarise(
      skill1_r = cor(mean_accuracy, skill1, use = "complete.obs"),
      skill2_r = cor(mean_accuracy, skill2, use = "complete.obs"),
      skill3_r = cor(mean_accuracy, skill3, use = "complete.obs")
    )
  
  # Reshape data for plotting and remove `_r` suffix
  skill_correlations_long <- skill_correlations %>%
    pivot_longer(cols = starts_with("skill"), names_to = "skill", values_to = "pearsons_r") %>%
    mutate(skill = str_remove(skill, "_r")) # Remove the "_r" suffix
  
  return(skill_correlations_long)
}

# Function to calculate correlations for all encounters in a level
calculate_all_correlations <- function(level, mean_accuracy_first, mean_accuracy_middle, mean_accuracy_last) {
  list(
    first = calculate_skill_correlations(level$first, mean_accuracy_first),
    middle = calculate_skill_correlations(level$middle, mean_accuracy_middle),
    last = calculate_skill_correlations(level$last, mean_accuracy_last)
  ) %>%
    bind_rows(.id = "encounter")
}

# Combine data from all levels and encounters
combine_correlation_data <- function(levels) {
  map_dfr(levels, ~ calculate_all_correlations(.x, .x$mean_accuracy_first, .x$mean_accuracy_middle, .x$mean_accuracy_last),
          .id = "level")
}

# Data setup for each level (replace these with your datasets)
level1 <- list(
  first = l1_first_3sk,
  middle = l1_middle_3sk,
  last = l1_last_3sk,
  mean_accuracy_first = mean_accuracy_level1_first,
  mean_accuracy_middle = mean_accuracy_level1_middle,
  mean_accuracy_last = mean_accuracy_level1_last
)

level2 <- list(
  first = l2_first_3sk,
  middle = l2_middle_3sk,
  last = l2_last_3sk,
  mean_accuracy_first = mean_accuracy_level2_first,
  mean_accuracy_middle = mean_accuracy_level2_middle,
  mean_accuracy_last = mean_accuracy_level2_last
)

level3 <- list(
  first = l3_first_3sk,
  middle = l3_middle_3sk,
  last = l3_last_3sk,
  mean_accuracy_first = mean_accuracy_level3_first,
  mean_accuracy_middle = mean_accuracy_level3_middle,
  mean_accuracy_last = mean_accuracy_level3_last
)

# Calculate correlations for all levels
all_correlations <- combine_correlation_data(list(level1, level2, level3))

# Ensure encounters are ordered logically: "first", "middle", "last"
all_correlations <- all_correlations %>%
  mutate(encounter = factor(encounter, levels = c("first", "middle", "last")))

# Plot correlations using a facet grid and remove legend
ggplot(all_correlations, aes(x = skill, y = pearsons_r, fill = skill)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  facet_grid(level ~ encounter) +
  theme_minimal() +
  labs(
    title = "Correlations Between Skill Probabilities and Mean Accuracy",
    x = "Skill",
    y = "Pearson's r"
  ) +
  scale_fill_brewer(palette = "Set3", guide = "none")
```
# Calculate and plot correlations between skill probabilties and reaction time
```{r}
# Function to calculate correlations between skill probabilities and reaction time
calculate_reaction_time_correlations <- function(graaftel_data, encounter_data) {
  # Ensure cue_text alignment
  graaftel_data <- graaftel_data %>%
    mutate(cue_text = str_trim(cue_text))
  encounter_data <- encounter_data %>%
    mutate(cue_text = str_trim(cue_text))

  # Remove rows with missing cue_text
  graaftel_data <- graaftel_data %>%
    filter(!is.na(cue_text))
  encounter_data <- encounter_data %>%
    filter(!is.na(cue_text))

  # Summarise reaction time by cue_text
  reaction_time_data <- encounter_data %>%
    group_by(cue_text) %>%
    summarise(mean_reaction_time = mean(reaction_time, na.rm = TRUE), .groups = "drop")

  # Join GraafTel data with reaction time data on "cue_text"
  merged_data <- graaftel_data %>%
    left_join(reaction_time_data, by = "cue_text")

  # Calculate Pearson's r for each skill
  skill_correlations <- merged_data %>%
    summarise(
      skill1_r = cor(mean_reaction_time, skill1, use = "complete.obs"),
      skill2_r = cor(mean_reaction_time, skill2, use = "complete.obs"),
      skill3_r = cor(mean_reaction_time, skill3, use = "complete.obs")
    )

  # Reshape data for plotting
  skill_correlations_long <- skill_correlations %>%
    pivot_longer(cols = starts_with("skill"), names_to = "skill", values_to = "pearsons_r")

  return(skill_correlations_long)
}

# Combine all correlation data into a single dataframe
combine_correlation_data <- function(level, encounter, graaftel_data, encounter_data) {
  correlations <- calculate_reaction_time_correlations(graaftel_data, encounter_data)
  correlations <- correlations %>%
    mutate(level = level, encounter = encounter)
  return(correlations)
}

# Create a consolidated dataframe for all levels and encounters
correlation_results <- bind_rows(
  combine_correlation_data("Level 2", "First", s3l2first, dat_level2_first),
  combine_correlation_data("Level 2", "Middle", s3l2middle, dat_level2_middle),
  combine_correlation_data("Level 2", "Last", s3l2last, dat_level2_last),
  combine_correlation_data("Level 3", "First", s3l3first, dat_level3_first),
  combine_correlation_data("Level 3", "Middle", s3l3middle, dat_level3_middle),
  combine_correlation_data("Level 3", "Last", s3l3last, dat_level3_last)
)

# Ensure proper ordering of encounters
correlation_results <- correlation_results %>%
  mutate(encounter = factor(encounter, levels = c("First", "Middle", "Last")))

# Replace skill labels in the dataframe
correlation_results <- correlation_results %>%
  mutate(skill = recode(skill,
                        "skill1_r" = "1",
                        "skill2_r" = "2",
                        "skill3_r" = "3"))

# Plot Pearson's r for all levels and encounters using a facet grid
ggplot(correlation_results, aes(x = encounter, y = skill, fill = pearsons_r, label = round(pearsons_r, 2))) +
  geom_tile() +
  geom_text() +
  facet_wrap(vars(level)) +
  scale_fill_gradient2(
    limits = c(-1, 1),
    midpoint = 0,
    low = '#4575b4',   # Softer blue
    mid = '#f7f7f7',   # Neutral white
    high = '#d73027',  # Softer red
    guide = guide_colorbar(
      title = "Pearson\nCorrelation",
      title.position = "right",
      barheight = unit(8.4, "cm")  # Adjust legend height
    ),
    aesthetics = 'fill',
    breaks = c(-1, -0.5, 0, 0.5, 1), # Set legend breaks to -1, 0, and 1
  ) +
  theme_minimal() +
  labs(
    title = "Skill Probabilities vs Mean Reaction Time Across Levels and Encounters",
    x = "Level",
    y = "Skill"
  )
```
# Calculate and plot correlations between skill probabilties from L2 and memory decay (alpha) from L3
```{r}
# Function to calculate correlations between skill probabilities and memory decay rates (alpha)
calculate_alpha_correlations <- function(graaftel_data, encounter_data) {
  # Ensure cue_text alignment
  graaftel_data <- graaftel_data %>%
    mutate(cue_text = str_trim(cue_text))
  encounter_data <- encounter_data %>%
    mutate(cue_text = str_trim(cue_text))

  # Remove rows with missing cue_text
  graaftel_data <- graaftel_data %>%
    filter(!is.na(cue_text))
  encounter_data <- encounter_data %>%
    filter(!is.na(cue_text))

  # Summarise alpha (memory decay rate) by cue_text
  alpha_data <- encounter_data %>%
    group_by(cue_text) %>%
    summarise(mean_alpha = mean(alpha, na.rm = TRUE), .groups = "drop")

  # Join GraafTel data with alpha data on "cue_text"
  merged_data <- graaftel_data %>%
    left_join(alpha_data, by = "cue_text")

  # Calculate Pearson's r for each skill
  skill_correlations <- merged_data %>%
    summarise(
      skill1_r = cor(mean_alpha, skill1, use = "complete.obs"),
      skill2_r = cor(mean_alpha, skill2, use = "complete.obs"),
      skill3_r = cor(mean_alpha, skill3, use = "complete.obs")
    )

  # Reshape data for plotting
  skill_correlations_long <- skill_correlations %>%
    pivot_longer(cols = starts_with("skill"), names_to = "skill", values_to = "pearsons_r")

  return(skill_correlations_long)
}

# Combine all correlation data into a single dataframe for Level 2 skills and memory decay rates
combine_alpha_correlation_data <- function(encounter, graaftel_data, encounter_data) {
  correlations <- calculate_alpha_correlations(graaftel_data, encounter_data)
  correlations <- correlations %>%
    mutate(level = "Level 2", encounter = encounter)
  return(correlations)
}

# Create a consolidated dataframe for Level 2 correlations with memory decay rates
alpha_correlation_results <- bind_rows(
  combine_alpha_correlation_data("First", s3l2first, dat_level2_first),
  combine_alpha_correlation_data("Middle", s3l2middle, dat_level2_middle),
  combine_alpha_correlation_data("Last", s3l2last, dat_level2_last)
)

# Ensure proper ordering of encounters
alpha_correlation_results <- alpha_correlation_results %>%
  mutate(encounter = factor(encounter, levels = c("First", "Middle", "Last")))

# Replace skill labels in the dataframe
alpha_correlation_results <- alpha_correlation_results %>%
  mutate(skill = recode(skill,
                        "skill1_r" = "1",
                        "skill2_r" = "2",
                        "skill3_r" = "3"))

# Plot Pearson's r for Level 2 skills and memory decay rates using a facet grid
ggplot(alpha_correlation_results, aes(x = encounter, y = skill, fill = pearsons_r, label = round(pearsons_r, 2))) +
  geom_tile() +
  geom_text() +
  scale_fill_gradient2(
    limits = c(-1, 1),
    midpoint = 0,
    low = '#4575b4',
    mid = '#f7f7f7',
    high = '#d73027',
    guide = guide_colorbar(
      title = "Pearson\nCorrelation",
      title.position = "right",
      barheight = unit(8.4, "cm")
    ),
    aesthetics = 'fill',
    breaks = c(-1, -0.5, 0, 0.5, 1),
  ) +
  theme_minimal() +
  labs(
    title = "Level 2 Skills vs Memory Decay Rates (Alpha) Across Encounters",
    x = "Encounter",
    y = "Skill"
  )
```
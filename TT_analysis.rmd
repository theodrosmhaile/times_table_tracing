# TafelTrainer Data Analysis

```{r setup, include=FALSE}
# Load required libraries
library(anytime)
library(broom)
library(dplyr)
library(ggcorrplot)
library(ggplot2)
library(lubridate)
library(patchwork)
library(psych)
library(readr)
library(skimr)
library(stringr)
library(tidyverse)
```

## Description

This analysis prepares data from Levels 1, 2, and 3 of the TafelTrainer dataset. The goal is to create datasets for GraafTel input (first, middle, and last encounters) and conduct descriptive analyses.

# Data preparation

```{r}
# Load dataset
dat <- readRDS('./TT_data/tt_responses_all_clean.rds')

# Initial filtering: Ensure 'correct' is numeric and clean `cue_text`
dat <- dat %>%
  mutate(
    correct = as.numeric(correct),
    cue_text = str_replace_all(cue_text, "\\+", " ") # Replace "+" with a space
  )

# # Define valid responses (numeric values ≤ 100)
# valid_responses <- 0:100
# 
# # Filter for valid 'given_response'
# dat <- dat %>%
#   filter(as.numeric(given_response) %in% valid_responses)

# Add multiplier and multiplicand columns globally to the dataset
dat <- dat %>%
  mutate(
    multiplier = as.numeric(str_extract(cue_text, "^\\d+")), # Extract the first number in `cue_text`
    multiplicand = as.numeric(str_extract(cue_text, "\\d+$")) # Extract the second number in `cue_text`
  )

# Function to process each level and add encounter counter
prepare_level_data <- function(data, level_filter) {
  data %>%
    filter(level == level_filter) %>%
    arrange(user_id, cue_text, session_id) %>%
    group_by(user_id, cue_text) %>%
    mutate(encounter_num = row_number()) %>%
    ungroup()
}

# Prepare full datasets for Levels 1, 2, and 3 (including `encounter_num`)
dat_level1 <- prepare_level_data(dat, 1)
dat_level2 <- prepare_level_data(dat, 2)
dat_level3 <- prepare_level_data(dat, 3)

# Function to filter specific encounters dynamically
filter_encounter <- function(data, encounter_type) {
  data %>%
    group_by(user_id, cue_text) %>%
    filter(
      case_when(
        # First encounter
        encounter_type == "first" ~ encounter_num == 1,
        # Middle encounter: Use ceiling(n() / 2), but if it results in 1, use 2 if available
        encounter_type == "middle" ~ {
          middle_encounter <- ceiling(n() / 2)
          encounter_num == if_else(middle_encounter == 1 & n() > 1, 2L, middle_encounter)
        },
        # Last encounter
        encounter_type == "last" ~ encounter_num == n(),
        # Default: Exclude rows
        TRUE ~ FALSE
      )
    ) %>%
    ungroup()
}

# Generate encounter-specific datasets dynamically from the full datasets
dat_level1_first <- filter_encounter(dat_level1, "first")
dat_level1_middle <- filter_encounter(dat_level1, "middle")
dat_level1_last <- filter_encounter(dat_level1, "last")

dat_level2_first <- filter_encounter(dat_level2, "first")
dat_level2_middle <- filter_encounter(dat_level2, "middle")
dat_level2_last <- filter_encounter(dat_level2, "last")

dat_level3_first <- filter_encounter(dat_level3, "first")
dat_level3_middle <- filter_encounter(dat_level3, "middle")
dat_level3_last <- filter_encounter(dat_level3, "last")
```

# Define a custom ggplot theme

```{r}
custom_theme <- function() {
  theme_minimal() +
    theme(
      # Text sizing
      text = element_text(size = 18),
      axis.text.x = element_text(size = 16),
      axis.text.y = element_text(size = 16),
      axis.title.x = element_text(size = 18),
      axis.title.y = element_text(size = 18),
      plot.title = element_text(size = 20, hjust = 0),
      plot.subtitle = element_text(size = 18, hjust = 0, color = "grey30"),
      
      # Legend formatting
      legend.text = element_text(size = 14),
      legend.title = element_text(angle = 90, hjust = 0.5, size = 18),
      legend.key.height = unit(2, "cm"),
      legend.position = "right",
      
      # Grid
      panel.grid.major = element_line(color = "grey80", linetype = "dashed"),
      panel.grid.minor = element_blank(),
      
      # Facet labels
      strip.text = element_text(size = 12)
    )
}
```

# Verify outputs

```{r}
# Prepare data for verification
# Combine all levels and encounter types to verify overall distribution
verify_data <- bind_rows(
  dat_level1 %>% mutate(level == 1),
  dat_level2 %>% mutate(level == 2),
  dat_level3 %>% mutate(level == 3)
)

# Plot histogram for the distribution of responses
# Ensures that the data used for GraafTel matches expected values
# verify_data %>%
#   select(given_response, cue_text, level) %>%
#   filter(given_response %in% 0:100) %>%
#   mutate(given_response = as.numeric(given_response)) %>%
#   ggplot(aes(x = given_response, fill = factor(level))) +
#   geom_histogram(binwidth = 1, position = "dodge") +
#   theme_minimal() +
#   labs(
#     title = "Distribution of Responses Across Levels",
#     x = "Given Response",
#     y = "Count",
#     fill = "Level"
#   ) +
#   theme(axis.text.x = element_text(angle = 90)) +
#   scale_fill_brewer(palette = "Set3")

# Calculate mean accuracy for each fact_id and level
mean_accuracy_data <- verify_data %>%
  group_by(level, cue_text) %>% # Group by level and fact_id
  summarize(mean_accuracy = mean(correct), .groups = "drop") # Calculate mean accuracy

# Create a box plot of mean accuracies
mean_accuracy_data %>%
  ggplot(aes(x = factor(level), y = mean_accuracy, fill = factor(level))) +
  geom_boxplot(alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Distribution of Mean Accuracies by Level",
    x = "Level",
    y = "Mean Accuracy",
    fill = "Level"
  ) +
  scale_fill_brewer(palette = "Set3") +
  theme(legend.position = "none")
```

# Generate data for heatmaps

```{r}
# Function to generate descriptive statistics for heatmap
generate_descriptive_data <- function(data) {
  data %>%
    group_by(cue_text, multiplier, multiplicand) %>%
    summarise(
      mean_acc = mean(correct, na.rm = TRUE), # Calculate mean accuracy
      n = n(), # Count number of observations
      se = sd(correct, na.rm = TRUE) / sqrt(n()), # Calculate standard error
      .groups = "drop"
    )
}

# Generate `dat4plot` datasets for overall statistics (all encounters combined) per level
dat4plot_level1 <- generate_descriptive_data(dat_level1)
dat4plot_level2 <- generate_descriptive_data(dat_level2)
dat4plot_level3 <- generate_descriptive_data(dat_level3)

# Generate `dat4plot` datasets for each level and encounter
dat4plot_level1_first <- generate_descriptive_data(dat_level1_first)
dat4plot_level1_middle <- generate_descriptive_data(dat_level1_middle)
dat4plot_level1_last <- generate_descriptive_data(dat_level1_last)

dat4plot_level2_first <- generate_descriptive_data(dat_level2_first)
dat4plot_level2_middle <- generate_descriptive_data(dat_level2_middle)
dat4plot_level2_last <- generate_descriptive_data(dat_level2_last)

dat4plot_level3_first <- generate_descriptive_data(dat_level3_first)
dat4plot_level3_middle <- generate_descriptive_data(dat_level3_middle)
dat4plot_level3_last <- generate_descriptive_data(dat_level3_last)
```

# TT descriptive statistics

```{r}
df_heatmaps <- bind_rows(
  dat4plot_level1       %>% mutate(facet_label = "Level 1: Overall"),
  dat4plot_level1_first %>% mutate(facet_label = "Level 1: First"),
  dat4plot_level1_middle %>% mutate(facet_label = "Level 1: Middle"),
  dat4plot_level1_last  %>% mutate(facet_label = "Level 1: Last"),
  
  dat4plot_level2       %>% mutate(facet_label = "Level 2: Overall"),
  dat4plot_level2_first %>% mutate(facet_label = "Level 2: First"),
  dat4plot_level2_middle %>% mutate(facet_label = "Level 2: Middle"),
  dat4plot_level2_last  %>% mutate(facet_label = "Level 2: Last"),
  
  dat4plot_level3       %>% mutate(facet_label = "Level 3: Overall"),
  dat4plot_level3_first %>% mutate(facet_label = "Level 3: First"),
  dat4plot_level3_middle %>% mutate(facet_label = "Level 3: Middle"),
  dat4plot_level3_last  %>% mutate(facet_label = "Level 3: Last")
)

# Ensure proper ordering of facet labels
df_heatmaps <- df_heatmaps %>%
  mutate(
    facet_label = factor(
      facet_label,
      levels = c(
        "Level 1: First", 
        "Level 1: Middle", 
        "Level 1: Last", 
        "Level 1: Overall",
        "Level 2: First", 
        "Level 2: Middle", 
        "Level 2: Last", 
        "Level 2: Overall",
        "Level 3: First", 
        "Level 3: Middle", 
        "Level 3: Last", 
        "Level 3: Overall"
      ),
    )
  )

heatmap_facet <- ggplot(df_heatmaps, 
                      aes(x = multiplier, 
                          y = multiplicand, 
                          fill = mean_acc, 
                          label = round(mean_acc, 2))) +
  geom_tile() +
  geom_text(size = 1.2) +
  # Use regular scale with proper breaks instead of discrete scales
  scale_x_discrete(breaks = 1:10) +
  scale_y_discrete(breaks = 1:10) +
  scale_fill_gradient2(
    limits = c(0.5, 1),
    midpoint = 0.75,
    low = 'blue',
    mid = 'red',
    high = 'yellow',
    guide = guide_colorbar(
      title = "Mean Accuracy", 
      title.position = "right", 
      title.theme = element_text(angle = 90, hjust = 0.5, size = 14),
      barheight = unit(8, "cm")
    ),
    aesthetics = 'fill',
    breaks = seq(0.5, 1, by = 0.1)
  ) +
  facet_wrap(~ facet_label, ncol = 4) +
  custom_theme() +
  labs(
    title = "Mean Accuracy by Fact Across Levels and Encounters",
    x = "Multiplier",
    y = "Multiplicand"
  ) +
  theme(
    # Make axis text visible but still small
    axis.text.x = element_text(size = 7, color = "black"),
    axis.text.y = element_text(size = 7, color = "black"),
    plot.title = element_text(size = 18, hjust = 0)
  )

# Function to plot bar plots with error bars
plot_barplot <- function(dat, title) {
  dat <- dat %>%
    mutate(cue_text = as.factor(cue_text)) # Ensure cue_text is a factor

  dat %>%
    ggplot(aes(
      x = cue_text, y = mean_acc, ymax = mean_acc + se, ymin = mean_acc - se, fill = as.factor(multiplicand)
    )) +
    geom_bar(stat = 'identity') +
    geom_errorbar() +
    scale_x_discrete(
      breaks = levels(dat$cue_text)[seq(1, length(levels(dat$cue_text)), by = 10)] # Show every 10th label correctly
    ) +
    theme_minimal() +
    labs(
      title = title,
      x = "Cue Text (Multiplication Fact)",
      y = "Mean Accuracy",
      fill = "Multiplicand"
    ) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = 'top')
}


# Print the plot
print(heatmap_facet)

# # Bar Plots
# barplot_level1 <- plot_barplot(dat4plot_level1, "Bar Plot of Mean Accuracy (Level 1: Overall)")
# barplot_level1_first <- plot_barplot(dat4plot_level1_first, "Bar Plot of Mean Accuracy (Level 1: First)")
# barplot_level1_middle <- plot_barplot(dat4plot_level1_middle, "Bar Plot of Mean Accuracy (Level 1: Middle)")
# barplot_level1_last <- plot_barplot(dat4plot_level1_last, "Bar Plot of Mean Accuracy (Level 1: Last)")
# barplot_level2 <- plot_barplot(dat4plot_level2, "Bar Plot of Mean Accuracy (Level 2: Overall)")
# barplot_level2_first <- plot_barplot(dat4plot_level2_first, "Bar Plot of Mean Accuracy (Level 2: First)")
# barplot_level2_middle <- plot_barplot(dat4plot_level2_middle, "Bar Plot of Mean Accuracy (Level 2: Middle)")
# barplot_level2_last <- plot_barplot(dat4plot_level2_last, "Bar Plot of Mean Accuracy (Level 2: Last)")
# barplot_level3 <- plot_barplot(dat4plot_level3, "Bar Plot of Mean Accuracy (Level 3: Overall)")
# barplot_level3_first <- plot_barplot(dat4plot_level3_first, "Bar Plot of Mean Accuracy (Level 3: First)")
# barplot_level3_middle <- plot_barplot(dat4plot_level3_middle, "Bar Plot of Mean Accuracy (Level 3: Middle)")
# barplot_level3_last <- plot_barplot(dat4plot_level3_last, "Bar Plot of Mean Accuracy (Level 3: Last)")
# 
# barplot_level1
# barplot_level1_first
# barplot_level1_middle
# barplot_level1_last
# barplot_level2
# barplot_level2_first
# barplot_level2_middle
# barplot_level2_last
# barplot_level3
# barplot_level3_first
# barplot_level3_middle
# barplot_level3_last
```

# GT descriptive statistics
## Create mean accuracy datasets for each level

```{r}
mean_accuracy_level1 <- verify_data %>%
  filter(level == 1) %>% # Filter for Level 1 data only
  group_by(cue_text) %>% # Group by cue_text
  summarise(mean_accuracy = mean(correct), .groups = "drop") # Calculate mean accuracy

mean_accuracy_level2 <- verify_data %>%
  filter(level == 2) %>% # Filter for Level 2 data only
  group_by(cue_text) %>% # Group by cue_text
  summarise(mean_accuracy = mean(correct), .groups = "drop") # Calculate mean accuracy

mean_accuracy_level3 <- verify_data %>%
  filter(level == 3) %>% # Filter for Level 3 data only
  group_by(cue_text) %>% # Group by cue_text
  summarise(mean_accuracy = mean(correct), .groups = "drop") # Calculate mean accuracy

```

## Create mean accuracy datasets for each encounter

```{r}
# Function to calculate mean and SD for overall accuracy
calculate_overall_accuracy <- function(data) {
  data %>%
    summarise(
      mean_accuracy = mean(correct, na.rm = TRUE),
      sd_accuracy = sd(correct, na.rm = TRUE)
    )
}

# Function to calculate mean and SD grouped by first number (multiplicand) in cue_text
calculate_accuracy_by_multiplicand <- function(data) {
  data %>%
    mutate(multiplicand = as.integer(str_extract(cue_text, "^[0-9]+"))) %>%  # Extract first number
    group_by(multiplicand) %>%
    summarise(
      mean_accuracy = mean(correct, na.rm = TRUE),
      sd_accuracy = sd(correct, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    arrange(multiplicand)  # Ensure ordered output
}

# Function to calculate mean and SD grouped by second number (multiplier) in cue_text
calculate_accuracy_by_multiplier <- function(data) {
  data %>%
    mutate(multiplier = as.integer(str_extract(cue_text, "(?<=x )\\d+"))) %>%  # Extract second number
    group_by(multiplier) %>%
    summarise(
      mean_accuracy = mean(correct, na.rm = TRUE),
      sd_accuracy = sd(correct, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    arrange(multiplier)  # Ensure ordered output
}

# Function to calculate mean and SD grouped by encounter type
calculate_accuracy_by_encounter <- function(data) {
  data %>%
    group_by(encounter_type) %>%
    summarise(
      mean_accuracy = mean(correct, na.rm = TRUE),
      sd_accuracy = sd(correct, na.rm = TRUE),
      .groups = "drop"
    )
}

# Function to calculate mean and SD grouped by level
calculate_accuracy_by_level <- function(data) {
  data %>%
    group_by(level) %>%
    summarise(
      mean_accuracy = mean(correct, na.rm = TRUE),
      sd_accuracy = sd(correct, na.rm = TRUE),
      .groups = "drop"
    )
}

# NEW: Function to calculate mean and SD for each combination of level and encounter
calculate_accuracy_by_level_and_encounter <- function(data) {
  data %>%
    group_by(level, encounter_type) %>%
    summarise(
      mean_accuracy = mean(correct, na.rm = TRUE),
      sd_accuracy = sd(correct, na.rm = TRUE),
      .groups = "drop"
    )
}

# Combine all datasets into one for overall accuracy calculations
all_data <- bind_rows(
  dat_level1_first %>% mutate(level = 1, encounter_type = "first"),
  dat_level1_middle %>% mutate(level = 1, encounter_type = "middle"),
  dat_level1_last %>% mutate(level = 1, encounter_type = "last"),
  dat_level2_first %>% mutate(level = 2, encounter_type = "first"),
  dat_level2_middle %>% mutate(level = 2, encounter_type = "middle"),
  dat_level2_last %>% mutate(level = 2, encounter_type = "last"),
  dat_level3_first %>% mutate(level = 3, encounter_type = "first"),
  dat_level3_middle %>% mutate(level = 3, encounter_type = "middle"),
  dat_level3_last %>% mutate(level = 3, encounter_type = "last")
)

# Calculate overall mean and SD
overall_accuracy <- calculate_overall_accuracy(all_data)

# Calculate mean and SD grouped by multiplicand
accuracy_by_multiplicand <- calculate_accuracy_by_multiplicand(all_data)

# Calculate mean and SD grouped by multiplier
accuracy_by_multiplier <- calculate_accuracy_by_multiplier(all_data)

# Calculate mean and SD grouped by encounter type
accuracy_by_encounter <- calculate_accuracy_by_encounter(all_data)

# Calculate mean and SD grouped by level
accuracy_by_level <- calculate_accuracy_by_level(all_data)

# Calculate mean and SD grouped by level AND encounter
accuracy_by_level_and_encounter <- calculate_accuracy_by_level_and_encounter(all_data)

# Output results
print(overall_accuracy)
print(accuracy_by_multiplicand)
print(accuracy_by_multiplier)
print(accuracy_by_encounter)
print(accuracy_by_level)
print(accuracy_by_level_and_encounter)

# Example ggplot usage (unchanged, for reference)
ggplot() +
  geom_line(data = accuracy_by_multiplier,
            aes(x = multiplier, y = mean_accuracy, group = "Multiplier", color = "Multiplier"),
            linewidth = 1) +
  geom_point(data = accuracy_by_multiplier,
             aes(x = multiplier, y = mean_accuracy, group = "Multiplier", color = "Multiplier"),
             size = 3) +
  geom_line(data = accuracy_by_multiplicand,
            aes(x = multiplicand, y = mean_accuracy, group = "Multiplicand", color = "Multiplicand"),
            linewidth = 1) +
  geom_point(data = accuracy_by_multiplicand,
             aes(x = multiplicand, y = mean_accuracy, group = "Multiplicand", color = "Multiplicand"),
             size = 3) +
  labs(
    title = "Mean Accuracy Across Multiplication Fact Groups",
    x = "Number",
    y = "Mean Accuracy",
    color = "Group"
  ) +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(limits = c(0.8, 1)) +
  scale_color_manual(values = c("Multiplier" = "#4575b4", "Multiplicand" = "#d73027")) +
  theme_minimal() +
  custom_theme()
```

# Skill independence analysis

```{r}
compute_skill_correlation <- function(encounter_list, skill_count) {
  # Combine all encounters and calculate mean for each skill
  mean_skills <- map(encounter_list, ~ select(., num_range("skill", 1:skill_count))) %>%
    reduce(`+`) / length(encounter_list)
  
  # Compute correlation matrix
  corr_mat <- cor(mean_skills, use = "complete.obs")
  
  # Rename rows and columns of the correlation matrix
  rownames(corr_mat) <- 1:skill_count
  colnames(corr_mat) <- 1:skill_count
  
  # Create plot using ggcorrplot
  ggcorrplot(corr_mat,
             type = "lower",
             lab = TRUE,
             lab_size = 6,
             title = paste(skill_count, "-Skill Correlation"),
             ggtheme = theme_minimal(),
             colors = c("#4575b4", "#f7f7f7", "#d73027"),
             legend.title = "Pearson Correlation") +  # Explicit legend title
    labs(
      x = "Skill",                   
      y = "Skill"                   
    ) +
    theme(
      axis.text.x = element_text(size = 12, angle = 0, vjust = 1),
      axis.text.y = element_text(size = 12),                 
      axis.title.x = element_text(size = 14),                
      axis.title.y = element_text(size = 14, angle = 90),    
      legend.title = element_text(size = 14, angle = 90, vjust = 0.5, hjust = 0.5),
      legend.text = element_text(size = 12, angle = 0),                
      legend.key.height = unit(1.5, "cm"),                   
      legend.position = "right",
      plot.title = element_text(size = 16, hjust = 0.5)      
    ) +
    guides(fill = guide_colorbar(title.position = "right",
                                 title.hjust = 0.5,
                                 barheight = unit(1.5, "cm"),
                                 frame.colour = "black",
                                 ticks.colour = "black"))
}

# Calculate mean correlations for 3-6 skills across all encounters
plot_3_overall <- compute_skill_correlation(
  list(s3l2first, s3l2middle, s3l2last), 3
)

plot_4_overall <- compute_skill_correlation(
  list(s4l2first, s4l2middle, s4l2last), 4
)

plot_5_overall <- compute_skill_correlation(
  list(s5l2first, s5l2middle, s5l2last), 5
)

plot_6_overall <- compute_skill_correlation(
  list(s6l2first, s6l2middle, s6l2last), 6
)

# Print the plots
print(plot_3_overall)
print(plot_4_overall)
print(plot_5_overall)
print(plot_6_overall)
```

# Create datasets for GT ratings

```{r}
# Function to filter and clean GraafTel ratings output for a given number of skills (s)
filter_graaftel_data <- function(file_path, s) {
  num_cols <- 2 + s
  # Create a col_types specification: first two columns as character, remaining as double
  col_types_expr <- do.call(
    cols, 
    as.list(setNames(
      c(list(col_character(), col_character()), rep(list(col_double()), s)),
      paste0("X", 1:num_cols)
    ))
  )
  
  # Read the file with no header and the appropriate column types
  graaftel_data <- read_csv(file_path, col_names = FALSE, col_types = col_types_expr)
  
  # Set column names: "item", "cue_text", then "skill1", "skill2", ..., "skill{s}"
  colnames(graaftel_data) <- c("item", "cue_text", paste0("skill", 1:s))
  
  # Filter out rows with "student" in the 'item' column and trim spaces in 'cue_text'
  graaftel_data <- graaftel_data %>%
    filter(!str_detect(item, "student")) %>%
    select(-item) %>%
    mutate(cue_text = str_trim(cue_text, side = "both"))
  
  return(graaftel_data)
}

# Initialize an empty list to store all filtered datasets
all_filtered_graaftel_data <- list()

# Loop over each skill configuration from 3 to 6
for (s in 3:6) {
  current_path <- sprintf("./GT_ratings/GT_ratings_%ds", s)
  current_pattern <- sprintf("s%dl[1-3](first|middle|last).csv", s)
  
  file_paths <- list.files(
    path = current_path,
    pattern = current_pattern,
    full.names = TRUE
  )
  
  # Apply the filtering function with the corresponding number of skills
  filtered_data <- lapply(file_paths, filter_graaftel_data, s = s)
  
  # Name each element based on the file name (removing directory path and extension)
  names(filtered_data) <- gsub(".*/|\\.csv$", "", file_paths)
  
  # Combine the filtered data into the overall list
  all_filtered_graaftel_data <- c(all_filtered_graaftel_data, filtered_data)
}

# Assign each dataset to a separate variable in the global environment
for (dataset_name in names(all_filtered_graaftel_data)) {
  assign(dataset_name, all_filtered_graaftel_data[[dataset_name]])
}

# After running this code, the following variables are available:
# For 3-skill configurations: s3l1first, s3l1middle, s3l1last, s3l2first, s3l2middle, s3l2last, s3l3first, s3l3middle, s3l3last
# For 4-skill configurations: s4l1first, s4l1middle, s4l1last, s4l2first, s4l2middle, s4l2last, s4l3first, s4l3middle, s4l3last
# For 5-skill configurations: s5l1first, s5l1middle, s5l1last, s5l2first, s5l2middle, s5l2last, s5l3first, s5l3middle, s5l3last
# For 6-skill configurations: s6l1first, s6l1middle, s6l1last, s6l2first, s6l2middle, s6l2last, s6l3first, s6l3middle, s6l3last
```

# Calculate and plot correlations between skill probabilities and reaction time

```{r}
# Function to calculate correlations between skill probabilities and reaction time with significance testing
calculate_reaction_time_correlations <- function(graaftel_data, encounter_data) {
  # Ensure cue_text alignment
  graaftel_data <- graaftel_data %>%
    mutate(cue_text = str_trim(cue_text))
  encounter_data <- encounter_data %>%
    mutate(cue_text = str_trim(cue_text))

  # Remove rows with missing cue_text
  graaftel_data <- graaftel_data %>%
    filter(!is.na(cue_text))
  encounter_data <- encounter_data %>%
    filter(!is.na(cue_text))

  # Summarise reaction time by cue_text
  reaction_time_data <- encounter_data %>%
    group_by(cue_text) %>%
    summarise(mean_reaction_time = mean(reaction_time, na.rm = TRUE), .groups = "drop")

  # Join GraafTel data with reaction time data on "cue_text"
  merged_data <- graaftel_data %>%
    left_join(reaction_time_data, by = "cue_text")

  # Calculate Pearson's r with p-values and confidence intervals
  skill_correlations <- merged_data %>%
    summarise(
      skill1 = list(broom::tidy(cor.test(mean_reaction_time, skill1, method = "pearson"))),
      skill2 = list(broom::tidy(cor.test(mean_reaction_time, skill2, method = "pearson"))),
      skill3 = list(broom::tidy(cor.test(mean_reaction_time, skill3, method = "pearson")))
    ) %>%
    pivot_longer(cols = everything(), names_to = "skill", values_to = "test") %>%
    unnest(test) %>%
    select(skill, estimate, p.value, conf.low, conf.high) %>%
    rename(pearsons_r = estimate) %>%
    mutate(
      # Bonferroni correction for 3 tests per encounter
      p.value.adj = p.adjust(p.value, method = "bonferroni", n = 3),
      significance = case_when(
        p.value.adj < 0.001 ~ "***",
        p.value.adj < 0.01 ~ "**",
        p.value.adj < 0.05 ~ "*",
        TRUE ~ ""
      )
    )

  return(skill_correlations)
}

# Combine all correlation data into a single dataframe
combine_correlation_data <- function(level, encounter, graaftel_data, encounter_data) {
  correlations <- calculate_reaction_time_correlations(graaftel_data, encounter_data)
  correlations <- correlations %>%
    mutate(level = level, encounter = encounter)
  return(correlations)
}

# Create a consolidated dataframe for all levels and encounters
correlation_results <- bind_rows(
  combine_correlation_data("Level 2", "First", s3l2first, dat_level2_first),
  combine_correlation_data("Level 2", "Middle", s3l2middle, dat_level2_middle),
  combine_correlation_data("Level 2", "Last", s3l2last, dat_level2_last),
  combine_correlation_data("Level 3", "First", s3l3first, dat_level3_first),
  combine_correlation_data("Level 3", "Middle", s3l3middle, dat_level3_middle),
  combine_correlation_data("Level 3", "Last", s3l3last, dat_level3_last)
)

# Ensure proper ordering of encounters
correlation_results <- correlation_results %>%
  mutate(encounter = factor(encounter, levels = c("First", "Middle", "Last")))

# Replace skill labels in the dataframe
correlation_results <- correlation_results %>%
  mutate(skill = recode(skill,
                        "skill1" = "1",
                        "skill2" = "2",
                        "skill3" = "3"))

# Plot Pearson's r with significance stars
ggplot(correlation_results, aes(x = encounter, y = skill, fill = pearsons_r,
                              label = sprintf("%.2f%s", pearsons_r, significance))) +
  geom_tile(color = "white") +
  geom_text(size = 5.5, color = "black") +
  facet_wrap(vars(level)) +
  scale_fill_gradient2(
    limits = c(-1, 1),
    midpoint = 0,
    low = '#4575b4',
    mid = '#f7f7f7',
    high = '#d73027',
    guide = guide_colorbar(
      title = "Pearson Correlation", 
      title.position = "right", 
      title.theme = element_text(angle = 90, hjust = 0.5, size = 18),
      barheight = unit(8, "cm")
    ),
    breaks = c(-1, -0.5, 0, 0.5, 1)
  ) +
  custom_theme() +
  labs(
    title = "Skill Probabilities vs Mean Reaction Time",
    x = "Encounter",
    y = "Skill",
    caption = "Significance levels: *** p < 0.001; ** p < 0.01; * p < 0.05 (Bonferroni-adjusted)"
  ) +
  theme(
    panel.grid = element_blank(),
    plot.caption = element_text(size = 14, hjust = 0)
  )
```

# Check whether any students have not done level 3 problems

```{r}
# Identify unique students who have solved problems in Level 1
students_level1 <- dat %>%
  filter(level == 1) %>%
  distinct(user_id) %>%
  pull(user_id)

# Identify unique students who have solved problems in Level 3
students_level3 <- dat %>%
  filter(level == 3) %>%
  distinct(user_id) %>%
  pull(user_id)

# Find students who are in Level 1 but not in Level 3
students_only_level1 <- setdiff(students_level1, students_level3)
# Find students who are in Level 3 but not in Level 1
students_only_level3 <- setdiff(students_level3, students_level1)

students_only_level1
students_only_level3

# Create dat_level1_finishers by filtering out students who only did Level 1
dat_level1_finishers <- dat_level1 %>%
  filter(!user_id %in% students_only_level1)

# Create dat_level3_finishers by filtering out students who only did Level 3
dat_level3_finishers <- dat_level3 %>%
  filter(!user_id %in% students_only_level3)

dat_level1_finishers_first <- filter_encounter(dat_level1_finishers, "first")
dat_level1_finishers_middle <- filter_encounter(dat_level1_finishers, "middle")
dat_level1_finishers_last <- filter_encounter(dat_level1_finishers, "last")

dat_level3_finishers_first <- filter_encounter(dat_level3_finishers, "first")
dat_level3_finishers_middle <- filter_encounter(dat_level3_finishers, "middle")
dat_level3_finishers_last <- filter_encounter(dat_level3_finishers, "last")
```

# Calculate and plot correlations between item-level skill probabilties from L1 and their memory decay (alpha) from L3
```{r}
# Function to calculate correlations between Level 1 skill probabilities and Level 3 memory decay rates (alpha)
# with significance testing and Bonferroni correction
calculate_alpha_correlations <- function(graaftel_data, encounter_data) {
  graaftel_data <- graaftel_data %>%
    mutate(cue_text = str_trim(cue_text))
  encounter_data <- encounter_data %>%
    mutate(cue_text = str_trim(cue_text))
  
  # Remove rows with missing cue_text
  graaftel_data <- graaftel_data %>%
    filter(!is.na(cue_text))
  encounter_data <- encounter_data %>%
    filter(!is.na(cue_text))
  
  # Summarise alpha (memory decay rate) by cue_text
  alpha_data <- encounter_data %>%
    group_by(cue_text) %>%
    summarise(mean_alpha = mean(alpha, na.rm = TRUE), .groups = "drop")
  
  # Join GraafTel data with alpha data on "cue_text"
  merged_data <- graaftel_data %>%
    left_join(alpha_data, by = "cue_text")
  
  # Calculate Pearson's r with p-values and confidence intervals
  skill_correlations <- merged_data %>%
    summarise(
      skill1 = list(broom::tidy(cor.test(mean_alpha, skill1, method = "pearson"))),
      skill2 = list(broom::tidy(cor.test(mean_alpha, skill2, method = "pearson"))),
      skill3 = list(broom::tidy(cor.test(mean_alpha, skill3, method = "pearson")))
    ) %>%
    pivot_longer(cols = everything(), names_to = "skill", values_to = "test") %>%
    unnest(test) %>%
    select(skill, estimate, p.value, conf.low, conf.high) %>%
    rename(pearsons_r = estimate) %>%
    mutate(
      # Bonferroni correction for 3 tests per encounter
      p.value.adj = p.adjust(p.value, method = "bonferroni", n = 3),
      significance = case_when(
        p.value.adj < 0.001 ~ "***",
        p.value.adj < 0.01 ~ "**",
        p.value.adj < 0.05 ~ "*",
        TRUE ~ ""
      )
    )
  
  return(skill_correlations)
}

# Combine all correlation data into a single dataframe for Level 1 skills and Level 3 memory decay rates
combine_alpha_correlation_data <- function(encounter, graaftel_data, encounter_data) {
  correlations <- calculate_alpha_correlations(graaftel_data, encounter_data)
  correlations <- correlations %>%
    mutate(level = "Level 1 to Level 3", encounter = encounter)
  return(correlations)
}

# Create a consolidated dataframe for Level 1 skills vs Level 3 memory decay rates correlations
alpha_correlation_results <- bind_rows(
  combine_alpha_correlation_data("First", finishers_s3l1first, dat_level3_finishers_first),
  combine_alpha_correlation_data("Middle", finishers_s3l1middle, dat_level3_finishers_middle),
  combine_alpha_correlation_data("Last", finishers_s3l1last, dat_level3_finishers_last)
)

# Ensure proper ordering of encounters
alpha_correlation_results <- alpha_correlation_results %>%
  mutate(encounter = factor(encounter, levels = c("First", "Middle", "Last")))

# Replace skill labels in the dataframe
alpha_correlation_results <- alpha_correlation_results %>%
  mutate(skill = recode(skill,
                        "skill1" = "1",
                        "skill2" = "2",
                        "skill3" = "3"))

# Plot Pearson's r with significance stars
ggplot(alpha_correlation_results, aes(x = encounter, y = skill, fill = pearsons_r,
                                      label = sprintf("%.2f%s", pearsons_r, significance))) +
  geom_tile(color = "white") +
  geom_text(size = 5.5, color = "black") +
  scale_fill_gradient2(
    limits = c(-1, 1),
    midpoint = 0,
    low = '#4575b4',
    mid = '#f7f7f7',
    high = '#d73027',
    guide = guide_colorbar(
      title = "Pearson Correlation", 
      title.position = "right", 
      title.theme = element_text(angle = 90, hjust = 0.5, size = 18),
      barheight = unit(8, "cm")
    ),
    breaks = c(-1, -0.5, 0, 0.5, 1)
  ) +
  custom_theme() +
  labs(
    title = "Level 1 Skills vs Level 3 Memory Decay Rates (α)",
    subtitle = "Items Across Encounters",
    x = "Encounter",
    y = "Skill",
    caption = "Significance levels: *** p < 0.001; ** p < 0.01; * p < 0.05 (Bonferroni-adjusted)"
  ) +
  theme(
    panel.grid = element_blank(),
    plot.caption = element_text(size = 14, hjust = 0)
  )
```


# Calculate and plot correlations between student-level skill probabilitiessig from L1 and their memory decay (alpha) from L3
```{r}
# Function to extract and analyze student-level data from GraafTel outputs
filter_graaftel_student_data <- function(file_path, s) {
  num_cols <- 2 + s
  # Create a col_types specification: first two columns as character, remaining as double
  col_types_expr <- do.call(
    cols, 
    as.list(setNames(
      c(list(col_character(), col_character()), rep(list(col_double()), s)),
      paste0("X", 1:num_cols)
    ))
  )
  
  # Read the file with no header and the appropriate column types
  graaftel_data <- read_csv(file_path, col_names = FALSE, col_types = col_types_expr)
  
  # Set column names: "item", "student_id", then "skill1", "skill2", ..., "skill{s}"
  colnames(graaftel_data) <- c("item", "student_id", paste0("skill", 1:s))
  
  # Filter to ONLY include rows with "student" in the 'item' column
  graaftel_data <- graaftel_data %>%
    filter(str_detect(item, "student")) %>%
    select(-item) %>%
    mutate(
      student_id = str_trim(student_id, side = "both"),
      # Convert student_id to numeric to match TafelTrainer data format
      student_id = as.numeric(student_id)
    )
  
  return(graaftel_data)
}

# Initialize lists to store student skill data for Level 1 and Level 3
student_l1_skill_data <- list()
student_l3_skill_data <- list()

# Process Level 1 student data files
l1_file_paths <- list.files(
  path = "./GT_ratings/finishers13",
  pattern = "s3l1(first|middle|last).csv",
  full.names = TRUE
)

for(file_path in l1_file_paths) {
  encounter_type <- str_extract(file_path, "(first|middle|last)")
  student_l1_skill_data[[encounter_type]] <- filter_graaftel_student_data(file_path, 3)
}

# Process Level 3 student data files
l3_file_paths <- list.files(
  path = "./GT_ratings/finishers13",
  pattern = "s3l3(first|middle|last).csv",
  full.names = TRUE
)

for(file_path in l3_file_paths) {
  encounter_type <- str_extract(file_path, "(first|middle|last)")
  student_l3_skill_data[[encounter_type]] <- filter_graaftel_student_data(file_path, 3)
}

# Calculate student-level average memory decay rates for Level 3
student_alpha_data <- list()

# First encounter
student_alpha_data[["first"]] <- dat_level3_finishers %>%
  filter(encounter_num == 1) %>%
  group_by(user_id) %>%
  summarise(mean_alpha = mean(alpha, na.rm = TRUE), .groups = "drop") %>%
  rename(student_id = user_id)

# Middle encounter
student_alpha_data[["middle"]] <- dat_level3_finishers %>%
  group_by(user_id, cue_text) %>%
  mutate(mid_encounter = ceiling(max(encounter_num) / 2)) %>%
  filter(encounter_num == mid_encounter) %>%
  ungroup() %>%
  group_by(user_id) %>%
  summarise(mean_alpha = mean(alpha, na.rm = TRUE), .groups = "drop") %>%
  rename(student_id = user_id)

# Last encounter
student_alpha_data[["last"]] <- dat_level3_finishers %>%
  group_by(user_id, cue_text) %>%
  filter(encounter_num == max(encounter_num)) %>%
  ungroup() %>%
  group_by(user_id) %>%
  summarise(mean_alpha = mean(alpha, na.rm = TRUE), .groups = "drop") %>%
  rename(student_id = user_id)

# Function to calculate correlations between student Level 1 skills and Level 3 memory decay rates
# with significance testing and Bonferroni correction
calculate_student_alpha_correlations <- function(skill_data, alpha_data) {
  # Join skill data with alpha data on student_id
  merged_data <- skill_data %>%
    inner_join(alpha_data, by = "student_id")
  
  # Calculate Pearson's r with p-values and confidence intervals
  skill_correlations <- merged_data %>%
    summarise(
      skill1 = list(broom::tidy(cor.test(mean_alpha, skill1, method = "pearson"))),
      skill2 = list(broom::tidy(cor.test(mean_alpha, skill2, method = "pearson"))),
      skill3 = list(broom::tidy(cor.test(mean_alpha, skill3, method = "pearson")))
    ) %>%
    pivot_longer(cols = everything(), names_to = "skill", values_to = "test") %>%
    unnest(test) %>%
    select(skill, estimate, p.value, conf.low, conf.high) %>%
    rename(pearsons_r = estimate) %>%
    mutate(
      # Bonferroni correction for 3 tests per encounter
      p.value.adj = p.adjust(p.value, method = "bonferroni", n = 3),
      significance = case_when(
        p.value.adj < 0.001 ~ "***",
        p.value.adj < 0.01 ~ "**",
        p.value.adj < 0.05 ~ "*",
        TRUE ~ ""
      )
    )
  
  return(skill_correlations)
}

# Create a consolidated dataframe for student-level correlations
student_alpha_correlation_results <- bind_rows(
  calculate_student_alpha_correlations(student_l1_skill_data[["first"]], student_alpha_data[["first"]]) %>% 
    mutate(encounter = "First"),
  calculate_student_alpha_correlations(student_l1_skill_data[["middle"]], student_alpha_data[["middle"]]) %>% 
    mutate(encounter = "Middle"),
  calculate_student_alpha_correlations(student_l1_skill_data[["last"]], student_alpha_data[["last"]]) %>% 
    mutate(encounter = "Last")
)

# Ensure proper ordering of encounters and skills
student_alpha_correlation_results <- student_alpha_correlation_results %>%
  mutate(
    encounter = factor(encounter, levels = c("First", "Middle", "Last")),
    skill = as.numeric(str_extract(skill, "\\d+"))
  )

# Plot student-level correlations
ggplot(student_alpha_correlation_results, aes(x = encounter, y = skill, fill = pearsons_r,
                                              label = sprintf("%.2f%s", pearsons_r, significance))) +
  geom_tile(color = "white") +
  geom_text(size = 5.5, color = "black") +
  scale_fill_gradient2(
    limits = c(-1, 1),
    midpoint = 0,
    low = '#4575b4',
    mid = '#f7f7f7',
    high = '#d73027',
    guide = guide_colorbar(
      title = "Pearson Correlation", 
      title.position = "right", 
      title.theme = element_text(angle = 90, hjust = 0.5, size = 18),
      barheight = unit(8, "cm")
    ),
    breaks = c(-1, -0.5, 0, 0.5, 1)
  ) +
  custom_theme() +
  labs(
    title = "Level 1 Skills vs Level 3 Memory Decay Rates (α)",
    subtitle = "Students Across Encounters",
    x = "Encounter",
    y = "Skill",
    caption = "Significance levels: *** p < 0.001; ** p < 0.01; * p < 0.05 (Bonferroni-adjusted)"
  ) +
  theme(
    panel.grid = element_blank(),
    plot.caption = element_text(size = 14, hjust = 0)
  )
```
# Export data for GT

```{r}
# Function to export datasets
export_graaftel_data <- function(data, level, encounter_type) {
  # Construct the filename
  file_name <- paste0("./gt_in_l", level, encounter_type, ".csv")
  # Select the required columns
  data_to_export <- data %>%
    select(user_id, cue_text, correct)
  # Write data to CSV (without column headers)
  write.table(data_to_export, file_name, row.names = FALSE, col.names = FALSE, sep = ",")
}

# # Export data for Level 1
# export_graaftel_data(dat_level1_first, 1, "first")
# export_graaftel_data(dat_level1_middle, 1, "middle")
# export_graaftel_data(dat_level1_last, 1, "last")
# 
# # Export data for Level 2
# export_graaftel_data(dat_level2_first, 2, "first")
# export_graaftel_data(dat_level2_middle, 2, "middle")
# export_graaftel_data(dat_level2_last, 2, "last")
# 
# # Export data for Level 3
# export_graaftel_data(dat_level3_first, 3, "first")
# export_graaftel_data(dat_level3_middle, 3, "middle")
# export_graaftel_data(dat_level3_last, 3, "last")

# export_graaftel_data(dat_level1_finishers_first, 1, "first")
# export_graaftel_data(dat_level1_finishers_middle, 1, "middle")
# export_graaftel_data(dat_level1_finishers_last, 1, "last")
# 
# export_graaftel_data(dat_level3_finishers_first, 3, "first")
# export_graaftel_data(dat_level3_finishers_middle, 3, "middle")
# export_graaftel_data(dat_level3_finishers_last, 3, "last")
```

# Abandoned
## Calculate and plot correlations between skill probabilties and mean accuracy

```{r}
# Function to calculate correlations between skill probabilities and mean accuracy
calculate_skill_correlations <- function(graaftel_data, mean_accuracy_data) {
  # Join GraafTel data with mean accuracy data on "cue_text"
  merged_data <- graaftel_data %>%
    left_join(mean_accuracy_data, by = "cue_text")
  
  # Calculate Pearson's r for each skill
  skill_correlations <- merged_data %>%
    summarise(
      skill1_r = cor(mean_accuracy, skill1, use = "complete.obs"),
      skill2_r = cor(mean_accuracy, skill2, use = "complete.obs"),
      skill3_r = cor(mean_accuracy, skill3, use = "complete.obs")
    )
  
  # Reshape data for plotting and remove `_r` suffix
  skill_correlations_long <- skill_correlations %>%
    pivot_longer(cols = starts_with("skill"), names_to = "skill", values_to = "pearsons_r") %>%
    mutate(skill = str_remove(skill, "_r")) # Remove the "_r" suffix
  
  return(skill_correlations_long)
}

# Function to calculate correlations for all encounters in a level
calculate_all_correlations <- function(level, mean_accuracy_first, mean_accuracy_middle, mean_accuracy_last) {
  list(
    first = calculate_skill_correlations(level$first, mean_accuracy_first),
    middle = calculate_skill_correlations(level$middle, mean_accuracy_middle),
    last = calculate_skill_correlations(level$last, mean_accuracy_last)
  ) %>%
    bind_rows(.id = "encounter")
}

# Combine data from all levels and encounters
combine_correlation_data <- function(levels) {
  map_dfr(levels, ~ calculate_all_correlations(.x, .x$mean_accuracy_first, .x$mean_accuracy_middle, .x$mean_accuracy_last),
          .id = "level")
}

# Data setup for each level (replace these with your datasets)
level1 <- list(
  first = s3l1first,
  middle = s3l1middle,
  last = s3l1last,
  mean_accuracy_first = mean_accuracy_level1_first,
  mean_accuracy_middle = mean_accuracy_level1_middle,
  mean_accuracy_last = mean_accuracy_level1_last
)

level2 <- list(
  first = s3l2first,
  middle = s3l2middle,
  last = s3l2last,
  mean_accuracy_first = mean_accuracy_level2_first,
  mean_accuracy_middle = mean_accuracy_level2_middle,
  mean_accuracy_last = mean_accuracy_level2_last
)

level3 <- list(
  first = s3l3first,
  middle = s3l3middle,
  last = s3l3last,
  mean_accuracy_first = mean_accuracy_level3_first,
  mean_accuracy_middle = mean_accuracy_level3_middle,
  mean_accuracy_last = mean_accuracy_level3_last
)

# Calculate correlations for all levels
all_correlations <- combine_correlation_data(list(level1, level2, level3))

# Ensure encounters are ordered logically: "first", "middle", "last"
all_correlations <- all_correlations %>%
  mutate(encounter = factor(encounter, levels = c("first", "middle", "last")))

# Plot correlations using a facet grid and remove legend
ggplot(all_correlations, aes(x = skill, y = pearsons_r, fill = skill)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  facet_grid(level ~ encounter) +
  theme_minimal() +
  labs(
    title = "Correlations Between Skill Probabilities and Mean Accuracy",
    x = "Skill",
    y = "Pearson's r"
  ) +
  scale_fill_brewer(palette = "Set3", guide = "none")
```
## Calculate correlations between fact groups
### Calculate correlations against 2s (L2)

```{r}
df_first <- s3l2first %>%
  mutate(encounter = "First")

df_middle <- s3l2middle %>%
  mutate(encounter = "Middle")

df_last <- s3l2last %>%
  mutate(encounter = "Last")

# df_all has columns: cue_text, skill1, skill2, skill3, encounter
df_all <- bind_rows(df_first, df_middle, df_last)

filter_for_number <- function(data, number) {
  # Looks for exact matches of 'number' in the cue_text (like "2" in "2 x 3")
  data %>%
    filter(str_detect(cue_text, paste0("\\b", number, "\\b")))
}

# Calculate mean skill usage by encounter
calc_means_by_encounter <- function(data) {
  data %>%
    group_by(encounter) %>%
    summarise(
      mean_skill1 = mean(skill1, na.rm = TRUE),
      mean_skill2 = mean(skill2, na.rm = TRUE),
      mean_skill3 = mean(skill3, na.rm = TRUE),
      .groups = "drop"
    )
}

compute_skill_correlations <- function(dfA, dfB, labelA, labelB) {
  # Join by `encounter` so we line up First, Middle, Last
  joined <- dfA %>%
    rename_with(~ paste0(.,"_", labelA), starts_with("mean_skill")) %>%
    inner_join(
      dfB %>% rename_with(~ paste0(.,"_", labelB), starts_with("mean_skill")),
      by = "encounter"
    )
  
  # Calculate correlation for each skill
  corr_s1 <- cor(joined[[paste0("mean_skill1_", labelA)]],
                 joined[[paste0("mean_skill1_", labelB)]],
                 use = "complete.obs")
  corr_s2 <- cor(joined[[paste0("mean_skill2_", labelA)]],
                 joined[[paste0("mean_skill2_", labelB)]],
                 use = "complete.obs")
  corr_s3 <- cor(joined[[paste0("mean_skill3_", labelA)]],
                 joined[[paste0("mean_skill3_", labelB)]],
                 use = "complete.obs")
  
  # Return a data frame with results
  tibble(
    comparison = paste0(labelA, "_vs_", labelB),
    skill = c("Skill1","Skill2","Skill3"),
    pearsons_r = c(corr_s1, corr_s2, corr_s3)
  )
}

# Generate Encounter-Means for the Groups of Interest
numbers_of_interest <- c("1","2","3","4","5","6","7","8","9","10")

# Create a named list that maps each group number to its summarized data
grouped_means <- list()
for(num in numbers_of_interest) {
  group_data <- df_all %>% 
    filter_for_number(num) %>%
    calc_means_by_encounter()
  grouped_means[[num]] <- group_data
}

# Compute Correlations for (2 vs 4), (2 vs 6), (2 vs 8), (2 vs 10) etc
corr_results <- bind_rows(
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["1"]],  "2s", "1s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["3"]],  "2s", "3s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["4"]],  "2s", "4s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["5"]],  "2s", "5s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["6"]],  "2s", "6s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["7"]],  "2s", "7s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["8"]],  "2s", "8s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["9"]],  "2s", "9s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["10"]], "2s", "10s")
)

# Print correlation results to console
print(corr_results)

# Heatmap Visualisation of Correlations
# Prepare data for heatmap
heatmap_data <- corr_results %>%
  mutate(
    comparison = gsub("2s_vs_", "", comparison),  # Simplify comparison labels
    skill = recode(skill, "Skill1" = "Skill 1", "Skill2" = "Skill 2", "Skill3" = "Skill 3"),
    comparison = factor(comparison, levels = c("1s", "3s", "4s", "5s", "6s", "7s", "8s", "9s", "10s"))
  )

heatmap_data <- heatmap_data %>%
  mutate(skill = as.numeric(str_extract(skill, "\\d+")))  # Extract the numeric part of 'Skill 1', 'Skill 2', etc.

# Create the heatmap
heatmap_plot <- ggplot(heatmap_data, aes(x = comparison, y = skill, fill = pearsons_r, label = round(pearsons_r, 2))) +
  geom_tile() +  # Create the heatmap tiles
  geom_text(size = 5) +  # Add Pearson's r as text in the tiles
  scale_fill_gradient2(
    low = "#4575b4",   # Blue for negative correlations
    mid = "#f7f7f7",   # White for neutral correlations
    high = "#d73027",  # Red for positive correlations
    midpoint = 0,      # Neutral midpoint
    limits = c(-1, 1), # Scale from -1 to 1
    breaks = seq(-1, 1, by = 0.5),  # Define legend breaks
    guide = guide_colorbar(
      title = "Pearson Correlation", # Define legend title
      title.position = "right",      # Position legend title
      title.theme = element_text(angle = 90, hjust = 0.5, size = 14), # Rotate and style the legend title
      barheight = unit(8.1, "cm")    # Adjust legend bar height
    )
  ) +
  theme_minimal() +
  theme(
    legend.title = element_text(angle = 90, hjust = 0.5) # Rotate and centre the legend title
  ) +
  labs(
    title = "Correlations between 2's and other groups across skills",
    x = "Comparison Group",
    y = "Skill"
  ) +
  theme(
    axis.text.x = element_text(size = 16),                  # Axis tick label size (x-axis)
    axis.text.y = element_text(size = 16),                  # Axis tick label size (y-axis)
    axis.title.x = element_text(size = 18),                # X-axis label size
    axis.title.y = element_text(size = 18),                # Y-axis label size
    legend.key.height = unit(2, "cm"),                     # Legend key height
    plot.title = element_text(size = 19, hjust = 0.5),     # Title size and centering
    panel.grid = element_blank(),                           # Remove gridlines
    legend.text = element_text(size = 14)
  )

# Print the heatmap
print(heatmap_plot)


```

### Calculate correlations against 3s (L2)

```{r}
# Compute Correlations for (3 vs 1), (3 vs 2), (3 vs 4), ..., (3 vs 10)
corr_results_3s <- bind_rows(
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["1"]],  "3s", "1s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["2"]],  "3s", "2s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["4"]],  "3s", "4s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["5"]],  "3s", "5s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["6"]],  "3s", "6s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["7"]],  "3s", "7s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["8"]],  "3s", "8s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["9"]],  "3s", "9s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["10"]], "3s", "10s")
)

# Print correlation results to console
print(corr_results_3s)

# Heatmap Visualisation of Correlations
# Prepare data for heatmap
heatmap_data_3s <- corr_results_3s %>%
  mutate(
    comparison = gsub("3s_vs_", "", comparison),  # Simplify comparison labels
    skill = recode(skill, "Skill1" = "Skill 1", "Skill2" = "Skill 2", "Skill3" = "Skill 3"),
    comparison = factor(comparison, levels = c("1s", "2s", "4s", "5s", "6s", "7s", "8s", "9s", "10s"))
  )

heatmap_data_3s <- heatmap_data_3s %>%
  mutate(skill = as.numeric(str_extract(skill, "\\d+")))

# Create the heatmap
heatmap_plot_3s <- ggplot(heatmap_data_3s, aes(x = comparison, y = skill, fill = pearsons_r, label = round(pearsons_r, 2))) +
  geom_tile() +  # Create the heatmap tiles
  geom_text(size = 5) +  # Add Pearson's r as text in the tiles
  scale_fill_gradient2(
    low = "#4575b4",   # Blue for negative correlations
    mid = "#f7f7f7",   # White for neutral correlations
    high = "#d73027",  # Red for positive correlations
    midpoint = 0,      # Neutral midpoint
    limits = c(-1, 1), # Scale from -1 to 1
    breaks = seq(-1, 1, by = 0.5),  # Define legend breaks
    guide = guide_colorbar(
      title = "Pearson Correlation", # Define legend title
      title.position = "right",      # Position legend title
      title.theme = element_text(angle = 90, hjust = 0.5, size = 14), # Rotate and style the legend title
      barheight = unit(8.1, "cm")    # Adjust legend bar height
    )
  ) +
  theme_minimal() +
  theme(
    legend.title = element_text(angle = 90, hjust = 0.5) # Rotate and centre the legend title
  ) +
  labs(
    title = "Correlations between 3's and other groups across skills",
    x = "Comparison Group",
    y = "Skill"
  ) +
  theme(
    axis.text.x = element_text(size = 16),                  # Axis tick label size (x-axis)
    axis.text.y = element_text(size = 16),                  # Axis tick label size (y-axis)
    axis.title.x = element_text(size = 18),                # X-axis label size
    axis.title.y = element_text(size = 18),                # Y-axis label size
    legend.key.height = unit(2, "cm"),                     # Legend key height
    plot.title = element_text(size = 19, hjust = 0.5),     # Title size and centering
    panel.grid = element_blank(),                           # Remove gridlines
    legend.text = element_text(size = 14)
  )

# Print the heatmap
print(heatmap_plot_3s)
```

### Calculate correlations against 1s (L2)

```{r}
# Compute Correlations for (1 vs 2), (1 vs 3), (1 vs 4), ..., (1 vs 10)
corr_results_1s <- bind_rows(
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["2"]],  "1s", "2s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["3"]],  "1s", "3s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["4"]],  "1s", "4s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["5"]],  "1s", "5s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["6"]],  "1s", "6s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["7"]],  "1s", "7s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["8"]],  "1s", "8s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["9"]],  "1s", "9s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["10"]], "1s", "10s")
)

# Print correlation results to console
print(corr_results_1s)

# Heatmap Visualisation of Correlations
# Prepare data for heatmap
heatmap_data_1s <- corr_results_1s %>%
  mutate(
    comparison = gsub("1s_vs_", "", comparison),  # Simplify comparison labels
    skill = recode(skill, "Skill1" = "Skill 1", "Skill2" = "Skill 2", "Skill3" = "Skill 3"),
    comparison = factor(comparison, levels = c("2s", "3s", "4s", "5s", "6s", "7s", "8s", "9s", "10s"))
  )

heatmap_data_1s <- heatmap_data_1s %>%
  mutate(skill = as.numeric(str_extract(skill, "\\d+")))  # Extract the numeric part of 'Skill 1', 'Skill 2', etc.

# Plot with updated y-axis
heatmap_plot_1s <- ggplot(heatmap_data_1s, aes(x = comparison, y = skill, fill = pearsons_r, label = round(pearsons_r, 2))) +
  geom_tile() +  # Create the heatmap tiles
  geom_text(size = 5) +  # Add Pearson's r as text in the tiles
  scale_fill_gradient2(
    low = "#4575b4",   # Blue for negative correlations
    mid = "#f7f7f7",   # White for neutral correlations
    high = "#d73027",  # Red for positive correlations
    midpoint = 0,      # Neutral midpoint
    limits = c(-1, 1), # Scale from -1 to 1
    breaks = seq(-1, 1, by = 0.5),  # Define legend breaks
    guide = guide_colorbar(
      title = "Pearson Correlation", # Define legend title
      title.position = "right",      # Position legend title
      title.theme = element_text(angle = 90, hjust = 0.5, size = 14), # Rotate and style the legend title
      barheight = unit(8.1, "cm")    # Adjust legend bar height
    )
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 16),                  # Axis tick label size (x-axis)
    axis.text.y = element_text(size = 16),                  # Axis tick label size (y-axis)
    axis.title.x = element_text(size = 18),                # X-axis label size
    axis.title.y = element_text(size = 18),                # Y-axis label size
    legend.key.height = unit(2, "cm"),                     # Legend key height
    plot.title = element_text(size = 19, hjust = 0.5),     # Title size and centering
    panel.grid = element_blank(),                           # Remove gridlines
    legend.text = element_text(size = 14)
  ) +
  labs(
    title = "Correlations between 1's and other groups across skills",
    x = "Comparison Group",
    y = "Skill"
  )

# Print the heatmap
print(heatmap_plot_1s)
```

# TafelTrainer Data Analysis

```{r setup, include=FALSE}
# Load required libraries
library(skimr)
library(tidyverse)
library(lubridate)
library(anytime)
library(ggcorrplot)
library(patchwork)
library(dplyr)
```

## Description

This analysis prepares data from Levels 1, 2, and 3 of the TafelTrainer dataset. The goal is to create datasets for GraafTel input (first, middle, and last encounters) and conduct descriptive analyses.

# Data Preparation
```{r}
# Load dataset
dat <- readRDS('./TT_data/tt_responses_all_clean.rds')

# Initial filtering: Ensure 'correct' is numeric and clean `cue_text`
dat <- dat %>%
  mutate(
    correct = as.numeric(correct),
    cue_text = str_replace_all(cue_text, "\\+", " ") # Replace "+" with a space
  )

# Define valid responses (numeric values â‰¤ 100)
valid_responses <- 0:100

# Filter for valid 'given_response'
dat <- dat %>%
  filter(as.numeric(given_response) %in% valid_responses)

# Add multiplier and multiplicand columns globally to the dataset
dat <- dat %>%
  mutate(
    multiplier = as.numeric(str_extract(cue_text, "^\\d+")), # Extract the first number in `cue_text`
    multiplicand = as.numeric(str_extract(cue_text, "\\d+$")) # Extract the second number in `cue_text`
  )

# Function to process each level and add encounter counter
prepare_level_data <- function(data, level_filter) {
  data %>%
    filter(level == level_filter) %>%
    arrange(user_id, cue_text, session_id) %>%
    group_by(user_id, cue_text) %>%
    mutate(encounter_num = row_number()) %>%
    ungroup()
}

# Prepare full datasets for Levels 1, 2, and 3 (including `encounter_num`)
dat_level1 <- prepare_level_data(dat, 1)
dat_level2 <- prepare_level_data(dat, 2)
dat_level3 <- prepare_level_data(dat, 3)

# Function to filter specific encounters dynamically
filter_encounter <- function(data, encounter_type) {
  data %>%
    group_by(user_id, cue_text) %>%
    filter(
      case_when(
        # First encounter
        encounter_type == "first" ~ encounter_num == 1,
        # Middle encounter: Use ceiling(n() / 2), but if it results in 1, use 2 if available
        encounter_type == "middle" ~ {
          middle_encounter <- ceiling(n() / 2)
          encounter_num == if_else(middle_encounter == 1 & n() > 1, 2L, middle_encounter)
        },
        # Last encounter
        encounter_type == "last" ~ encounter_num == n(),
        # Default: Exclude rows
        TRUE ~ FALSE
      )
    ) %>%
    ungroup()
}

# Generate encounter-specific datasets dynamically from the full datasets
dat_level1_first <- filter_encounter(dat_level1, "first")
dat_level1_middle <- filter_encounter(dat_level1, "middle")
dat_level1_last <- filter_encounter(dat_level1, "last")

dat_level2_first <- filter_encounter(dat_level2, "first")
dat_level2_middle <- filter_encounter(dat_level2, "middle")
dat_level2_last <- filter_encounter(dat_level2, "last")

dat_level3_first <- filter_encounter(dat_level3, "first")
dat_level3_middle <- filter_encounter(dat_level3, "middle")
dat_level3_last <- filter_encounter(dat_level3, "last")
```
# Export Data for GraafTel
```{r}
# Function to export datasets
export_graaftel_data <- function(data, level, encounter_type) {
  # Construct the filename
  file_name <- paste0("./gt_in_l", level, encounter_type, ".csv")
  # Select the required columns
  data_to_export <- data %>%
    select(user_id, cue_text, correct)
  # Write data to CSV (without column headers)
  write.table(data_to_export, file_name, row.names = FALSE, col.names = FALSE, sep = ",")
}

# Export data for Level 1
export_graaftel_data(dat_level1_first, 1, "first")
export_graaftel_data(dat_level1_middle, 1, "middle")
export_graaftel_data(dat_level1_last, 1, "last")

# Export data for Level 2
export_graaftel_data(dat_level2_first, 2, "first")
export_graaftel_data(dat_level2_middle, 2, "middle")
export_graaftel_data(dat_level2_last, 2, "last")

# Export data for Level 3
export_graaftel_data(dat_level3_first, 3, "first")
export_graaftel_data(dat_level3_middle, 3, "middle")
export_graaftel_data(dat_level3_last, 3, "last")

# export_graaftel_data(dat_level1_finishers_first, 1, "first")
# export_graaftel_data(dat_level1_finishers_middle, 1, "middle")
# export_graaftel_data(dat_level1_finishers_last, 1, "last")
# 
# export_graaftel_data(dat_level3_finishers_first, 3, "first")
# export_graaftel_data(dat_level3_finishers_middle, 3, "middle")
# export_graaftel_data(dat_level3_finishers_last, 3, "last")
```

# Verify Outputs
```{r}
# Prepare data for verification
# Combine all levels and encounter types to verify overall distribution
verify_data <- bind_rows(
  dat_level1 %>% mutate(level == 1),
  dat_level2 %>% mutate(level == 2),
  dat_level3 %>% mutate(level == 3)
)

# Plot histogram for the distribution of responses
# Ensures that the data used for GraafTel matches expected values
verify_data %>%
  select(given_response, cue_text, level) %>%
  filter(given_response %in% 0:100) %>%
  mutate(given_response = as.numeric(given_response)) %>%
  ggplot(aes(x = given_response, fill = factor(level))) +
  geom_histogram(binwidth = 1, position = "dodge") +
  theme_minimal() +
  labs(
    title = "Distribution of Responses Across Levels",
    x = "Given Response",
    y = "Count",
    fill = "Level"
  ) +
  theme(axis.text.x = element_text(angle = 90)) +
  scale_fill_brewer(palette = "Set3")

# Calculate mean accuracy for each fact_id and level
mean_accuracy_data <- verify_data %>%
  group_by(level, cue_text) %>% # Group by level and fact_id
  summarize(mean_accuracy = mean(correct), .groups = "drop") # Calculate mean accuracy

# Create a box plot of mean accuracies
mean_accuracy_data %>%
  ggplot(aes(x = factor(level), y = mean_accuracy, fill = factor(level))) +
  geom_boxplot(alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Distribution of Mean Accuracies by Level",
    x = "Level",
    y = "Mean Accuracy",
    fill = "Level"
  ) +
  scale_fill_brewer(palette = "Set3") +
  theme(legend.position = "none")
```
# Generate Data for Heatmaps
```{r}
# Function to generate descriptive statistics for heatmap
generate_descriptive_data <- function(data) {
  data %>%
    group_by(multiplier, multiplicand) %>%
    summarise(
      mean_acc = mean(correct, na.rm = TRUE), # Calculate mean accuracy
      n = n(), # Count number of observations
      se = sd(correct, na.rm = TRUE) / sqrt(n()), # Calculate standard error
      .groups = "drop"
    )
}

# Generate `dat4plot` datasets for overall statistics (all encounters combined) per level
dat4plot_level1 <- generate_descriptive_data(dat_level1)
dat4plot_level2 <- generate_descriptive_data(dat_level2)
dat4plot_level3 <- generate_descriptive_data(dat_level3)

# Generate `dat4plot` datasets for each level and encounter
dat4plot_level1_first <- generate_descriptive_data(dat_level1_first)
dat4plot_level1_middle <- generate_descriptive_data(dat_level1_middle)
dat4plot_level1_last <- generate_descriptive_data(dat_level1_last)

dat4plot_level2_first <- generate_descriptive_data(dat_level2_first)
dat4plot_level2_middle <- generate_descriptive_data(dat_level2_middle)
dat4plot_level2_last <- generate_descriptive_data(dat_level2_last)

dat4plot_level3_first <- generate_descriptive_data(dat_level3_first)
dat4plot_level3_middle <- generate_descriptive_data(dat_level3_middle)
dat4plot_level3_last <- generate_descriptive_data(dat_level3_last)
```
# Descriptive Statistics
```{r}
df_heatmaps <- bind_rows(
  dat4plot_level1       %>% mutate(facet_label = "Level 1: Overall"),
  dat4plot_level1_first %>% mutate(facet_label = "Level 1: First"),
  dat4plot_level1_middle %>% mutate(facet_label = "Level 1: Middle"),
  dat4plot_level1_last  %>% mutate(facet_label = "Level 1: Last"),
  
  dat4plot_level2       %>% mutate(facet_label = "Level 2: Overall"),
  dat4plot_level2_first %>% mutate(facet_label = "Level 2: First"),
  dat4plot_level2_middle %>% mutate(facet_label = "Level 2: Middle"),
  dat4plot_level2_last  %>% mutate(facet_label = "Level 2: Last"),
  
  dat4plot_level3       %>% mutate(facet_label = "Level 3: Overall"),
  dat4plot_level3_first %>% mutate(facet_label = "Level 3: First"),
  dat4plot_level3_middle %>% mutate(facet_label = "Level 3: Middle"),
  dat4plot_level3_last  %>% mutate(facet_label = "Level 3: Last")
)

# Ensure proper ordering of facet labels
df_heatmaps <- df_heatmaps %>%
  mutate(
    facet_label = factor(
      facet_label,
      levels = c(
        "Level 1: First", 
        "Level 1: Middle", 
        "Level 1: Last", 
        "Level 1: Overall",
        "Level 2: First", 
        "Level 2: Middle", 
        "Level 2: Last", 
        "Level 2: Overall",
        "Level 3: First", 
        "Level 3: Middle", 
        "Level 3: Last", 
        "Level 3: Overall"
      )
    )
  )

# Plot the heatmap
heatmap_facet <- ggplot(df_heatmaps, 
                        aes(x = multiplier, 
                            y = multiplicand, 
                            fill = mean_acc, 
                            label = round(mean_acc, 2))) +
  geom_tile() +
  geom_text(size = 1) +
  scale_fill_gradient2(
    limits = c(0.5, 1),
    midpoint = 0.75,
    low = 'blue',
    mid = 'red',
    high = 'yellow',
    guide = guide_colorbar(
      title = "Mean Accuracy", 
      title.position = "right", 
      title.theme = element_text(angle = 90, hjust = 0.5, size = 14), # Adjust legend title size
      barheight = unit(8, "cm")
    ),
    aesthetics = 'fill',
    breaks = seq(0.5, 1, by = 0.05)
  ) +
  facet_wrap(~ facet_label, ncol = 4) +  # Adjust ncol as desired
  theme_minimal() +
  labs(
    title = "Mean Accuracy Across Levels and Encounters",
    x = "Multiplier",
    y = "Multiplicand"
  ) +
  theme(
    axis.text.x = element_text(size = 8),                  # Axis tick label size (x-axis)
    axis.text.y = element_text(size = 8),                  # Axis tick label size (y-axis)
    axis.title.x = element_text(size = 14),                # X-axis label size
    axis.title.y = element_text(size = 14),                # Y-axis label size
    legend.title = element_text(size = 14),                # Legend title size
    legend.key.height = unit(2, "cm"),                     # Legend key height
    plot.title = element_text(size = 16, hjust = 0.5),     # Title size and centering
    strip.text = element_text(size = 12)                   # Facet label size
  )

# Print the plot
print(heatmap_facet)

barplot_level1
barplot_level1_first
barplot_level1_middle
barplot_level1_last
barplot_level2
barplot_level2_first
barplot_level2_middle
barplot_level2_last
barplot_level3
barplot_level3_first
barplot_level3_middle
barplot_level3_last
```

# GraafTel Data Export Analysis
# Create mean accuracy datasets for each level
```{r}
mean_accuracy_level1 <- verify_data %>%
  filter(level == 1) %>% # Filter for Level 1 data only
  group_by(cue_text) %>% # Group by cue_text
  summarise(mean_accuracy = mean(correct), .groups = "drop") # Calculate mean accuracy

mean_accuracy_level2 <- verify_data %>%
  filter(level == 2) %>% # Filter for Level 2 data only
  group_by(cue_text) %>% # Group by cue_text
  summarise(mean_accuracy = mean(correct), .groups = "drop") # Calculate mean accuracy

mean_accuracy_level3 <- verify_data %>%
  filter(level == 3) %>% # Filter for Level 3 data only
  group_by(cue_text) %>% # Group by cue_text
  summarise(mean_accuracy = mean(correct), .groups = "drop") # Calculate mean accuracy

```
# Create mean accuracy datasets for each encounter
```{r}
# Function to calculate mean accuracy from preprocessed datasets
calculate_mean_accuracy <- function(data) {
  data %>%
    group_by(cue_text) %>% # Group by cue_text
    summarise(mean_accuracy = mean(correct), .groups = "drop") # Calculate mean accuracy
}

# Generate mean accuracy datasets for Level 1
mean_accuracy_level1_first <- calculate_mean_accuracy(dat_level1_first)
mean_accuracy_level1_middle <- calculate_mean_accuracy(dat_level1_middle)
mean_accuracy_level1_last <- calculate_mean_accuracy(dat_level1_last)

# Generate mean accuracy datasets for Level 2
mean_accuracy_level2_first <- calculate_mean_accuracy(dat_level2_first)
mean_accuracy_level2_middle <- calculate_mean_accuracy(dat_level2_middle)
mean_accuracy_level2_last <- calculate_mean_accuracy(dat_level2_last)

# Generate mean accuracy datasets for Level 3
mean_accuracy_level3_first <- calculate_mean_accuracy(dat_level3_first)
mean_accuracy_level3_middle <- calculate_mean_accuracy(dat_level3_middle)
mean_accuracy_level3_last <- calculate_mean_accuracy(dat_level3_last)
```
# Create datasets for GraafTel ratings
```{r}
# Function to filter and clean GraafTel ratings output
filter_graaftel_data <- function(file_path) {
  graaftel_data <- read_csv(file_path, col_names = FALSE, col_types = cols(
    X1 = col_character(),
    X2 = col_character(),
    X3 = col_double(),
    X4 = col_double(),
    X5 = col_double(),
    X6 = col_double(),
    X7 = col_double(),
    X8 = col_double()
  ))

  # Rename columns for clarity
  colnames(graaftel_data) <- c("item", "cue_text", "skill1", "skill2", "skill3", "skill4", "skill5", "skill6")

  # Filter out rows containing "student" in the first column
  graaftel_data <- graaftel_data %>%
    filter(!str_detect(item, "student")) %>%
    select(-item)

  # Remove any leading spaces in cue_text
  graaftel_data <- graaftel_data %>%
    mutate(cue_text = str_trim(cue_text, side = "both"))

  return(graaftel_data)
}

# List of file paths for all GraafTel outputs
file_paths <- list.files(
  path = "./GT_ratings/GT_ratings_6s",
  pattern = "s6l[1-3](first|middle|last).csv",
  full.names = TRUE
)

# Apply the filtering function to all files
filtered_graaftel_data <- lapply(file_paths, filter_graaftel_data)

# Name the datasets based on file names without extensions
names(filtered_graaftel_data) <- gsub(".*/|\\.csv$", "", file_paths)

# Assign cleaned datasets to individual variables for the 9 cases
# s3l1first <- filtered_graaftel_data[["s3l1first"]]
# s3l1middle <- filtered_graaftel_data[["s3l1middle"]]
# s3ls1last <- filtered_graaftel_data[["s3l1last"]]
# 
# s3l2first <- filtered_graaftel_data[["s3l2first"]]
# s3l2middle <- filtered_graaftel_data[["s3l2middle"]]
# s3l2last <- filtered_graaftel_data[["s3l2last"]]
# 
# s3l3first <- filtered_graaftel_data[["s3l3first"]]
# s3l3middle <- filtered_graaftel_data[["s3l3middle"]]
# s3l3last <- filtered_graaftel_data[["s3l3last"]]

# s4l1first <- filtered_graaftel_data[["s4l1first"]]
# s4l1middle <- filtered_graaftel_data[["s4l1middle"]]
# s4ls1last <- filtered_graaftel_data[["s4l1last"]]
# 
# s4l2first <- filtered_graaftel_data[["s4l2first"]]
# s4l2middle <- filtered_graaftel_data[["s4l2middle"]]
# s4l2last <- filtered_graaftel_data[["s4l2last"]]
# 
# s4l3first <- filtered_graaftel_data[["s4l3first"]]
# s4l3middle <- filtered_graaftel_data[["s4l3middle"]]
# s4l3last <- filtered_graaftel_data[["s4l3last"]]

# s5l1first <- filtered_graaftel_data[["s5l1first"]]
# s5l1middle <- filtered_graaftel_data[["s5l1middle"]]
# s5ls1last <- filtered_graaftel_data[["s5l1last"]]
# 
# s5l2first <- filtered_graaftel_data[["s5l2first"]]
# s5l2middle <- filtered_graaftel_data[["s5l2middle"]]
# s5l2last <- filtered_graaftel_data[["s5l2last"]]
# 
# s5l3first <- filtered_graaftel_data[["s5l3first"]]
# s5l3middle <- filtered_graaftel_data[["s5l3middle"]]
# s5l3last <- filtered_graaftel_data[["s5l3last"]]

s6l1first <- filtered_graaftel_data[["s6l1first"]]
s6l1middle <- filtered_graaftel_data[["s6l1middle"]]
s6ls1last <- filtered_graaftel_data[["s6l1last"]]

s6l2first <- filtered_graaftel_data[["s6l2first"]]
s6l2middle <- filtered_graaftel_data[["s6l2middle"]]
s6l2last <- filtered_graaftel_data[["s6l2last"]]
```
# Calculate and plot correlations between skill probabilties and mean accuracy
```{r}
# Function to calculate correlations between skill probabilities and mean accuracy
calculate_skill_correlations <- function(graaftel_data, mean_accuracy_data) {
  # Join GraafTel data with mean accuracy data on "cue_text"
  merged_data <- graaftel_data %>%
    left_join(mean_accuracy_data, by = "cue_text")
  
  # Calculate Pearson's r for each skill
  skill_correlations <- merged_data %>%
    summarise(
      skill1_r = cor(mean_accuracy, skill1, use = "complete.obs"),
      skill2_r = cor(mean_accuracy, skill2, use = "complete.obs"),
      skill3_r = cor(mean_accuracy, skill3, use = "complete.obs")
    )
  
  # Reshape data for plotting and remove `_r` suffix
  skill_correlations_long <- skill_correlations %>%
    pivot_longer(cols = starts_with("skill"), names_to = "skill", values_to = "pearsons_r") %>%
    mutate(skill = str_remove(skill, "_r")) # Remove the "_r" suffix
  
  return(skill_correlations_long)
}

# Function to calculate correlations for all encounters in a level
calculate_all_correlations <- function(level, mean_accuracy_first, mean_accuracy_middle, mean_accuracy_last) {
  list(
    first = calculate_skill_correlations(level$first, mean_accuracy_first),
    middle = calculate_skill_correlations(level$middle, mean_accuracy_middle),
    last = calculate_skill_correlations(level$last, mean_accuracy_last)
  ) %>%
    bind_rows(.id = "encounter")
}

# Combine data from all levels and encounters
combine_correlation_data <- function(levels) {
  map_dfr(levels, ~ calculate_all_correlations(.x, .x$mean_accuracy_first, .x$mean_accuracy_middle, .x$mean_accuracy_last),
          .id = "level")
}

# Data setup for each level (replace these with your datasets)
level1 <- list(
  first = l1_first_3sk,
  middle = l1_middle_3sk,
  last = l1_last_3sk,
  mean_accuracy_first = mean_accuracy_level1_first,
  mean_accuracy_middle = mean_accuracy_level1_middle,
  mean_accuracy_last = mean_accuracy_level1_last
)

level2 <- list(
  first = l2_first_3sk,
  middle = l2_middle_3sk,
  last = l2_last_3sk,
  mean_accuracy_first = mean_accuracy_level2_first,
  mean_accuracy_middle = mean_accuracy_level2_middle,
  mean_accuracy_last = mean_accuracy_level2_last
)

level3 <- list(
  first = l3_first_3sk,
  middle = l3_middle_3sk,
  last = l3_last_3sk,
  mean_accuracy_first = mean_accuracy_level3_first,
  mean_accuracy_middle = mean_accuracy_level3_middle,
  mean_accuracy_last = mean_accuracy_level3_last
)

# Calculate correlations for all levels
all_correlations <- combine_correlation_data(list(level1, level2, level3))

# Ensure encounters are ordered logically: "first", "middle", "last"
all_correlations <- all_correlations %>%
  mutate(encounter = factor(encounter, levels = c("first", "middle", "last")))

# Plot correlations using a facet grid and remove legend
ggplot(all_correlations, aes(x = skill, y = pearsons_r, fill = skill)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  facet_grid(level ~ encounter) +
  theme_minimal() +
  labs(
    title = "Correlations Between Skill Probabilities and Mean Accuracy",
    x = "Skill",
    y = "Pearson's r"
  ) +
  scale_fill_brewer(palette = "Set3", guide = "none")
```
# Calculate and plot correlations between skill probabilties and reaction time
```{r}
# Function to calculate correlations between skill probabilities and reaction time
calculate_reaction_time_correlations <- function(graaftel_data, encounter_data) {
  # Ensure cue_text alignment
  graaftel_data <- graaftel_data %>%
    mutate(cue_text = str_trim(cue_text))
  encounter_data <- encounter_data %>%
    mutate(cue_text = str_trim(cue_text))

  # Remove rows with missing cue_text
  graaftel_data <- graaftel_data %>%
    filter(!is.na(cue_text))
  encounter_data <- encounter_data %>%
    filter(!is.na(cue_text))

  # Summarise reaction time by cue_text
  reaction_time_data <- encounter_data %>%
    group_by(cue_text) %>%
    summarise(mean_reaction_time = mean(reaction_time, na.rm = TRUE), .groups = "drop")

  # Join GraafTel data with reaction time data on "cue_text"
  merged_data <- graaftel_data %>%
    left_join(reaction_time_data, by = "cue_text")

  # Calculate Pearson's r for each skill
  skill_correlations <- merged_data %>%
    summarise(
      skill1_r = cor(mean_reaction_time, skill1, use = "complete.obs"),
      skill2_r = cor(mean_reaction_time, skill2, use = "complete.obs"),
      skill3_r = cor(mean_reaction_time, skill3, use = "complete.obs")
    )

  # Reshape data for plotting
  skill_correlations_long <- skill_correlations %>%
    pivot_longer(cols = starts_with("skill"), names_to = "skill", values_to = "pearsons_r")

  return(skill_correlations_long)
}

# Combine all correlation data into a single dataframe
combine_correlation_data <- function(level, encounter, graaftel_data, encounter_data) {
  correlations <- calculate_reaction_time_correlations(graaftel_data, encounter_data)
  correlations <- correlations %>%
    mutate(level = level, encounter = encounter)
  return(correlations)
}

# Create a consolidated dataframe for all levels and encounters
correlation_results <- bind_rows(
  combine_correlation_data("Level 2", "First", s3l2first, dat_level2_first),
  combine_correlation_data("Level 2", "Middle", s3l2middle, dat_level2_middle),
  combine_correlation_data("Level 2", "Last", s3l2last, dat_level2_last),
  combine_correlation_data("Level 3", "First", s3l3first, dat_level3_first),
  combine_correlation_data("Level 3", "Middle", s3l3middle, dat_level3_middle),
  combine_correlation_data("Level 3", "Last", s3l3last, dat_level3_last)
)

# Ensure proper ordering of encounters
correlation_results <- correlation_results %>%
  mutate(encounter = factor(encounter, levels = c("First", "Middle", "Last")))

# Replace skill labels in the dataframe
correlation_results <- correlation_results %>%
  mutate(skill = recode(skill,
                        "skill1_r" = "1",
                        "skill2_r" = "2",
                        "skill3_r" = "3"))

# Plot Pearson's r for all levels and encounters using a facet grid
ggplot(correlation_results, aes(x = encounter, y = skill, fill = pearsons_r, label = round(pearsons_r, 2))) +
  geom_tile() +
  geom_text(size = 6) + # Adjust text inside tiles
  facet_wrap(vars(level)) +
  scale_fill_gradient2(
    limits = c(-1, 1),
    midpoint = 0,
    low = '#4575b4',
    mid = '#f7f7f7',
    high = '#d73027',
    guide = guide_colorbar(
      title = "Pearson Correlation", 
      title.position = "right", 
      title.theme = element_text(angle = 90, hjust = 0.5, size = 14), # Rotate and adjust legend title size
      barheight = unit(8, "cm") # Adjust legend bar height
    ),
    aesthetics = 'fill',
    breaks = c(-1, -0.5, 0, 0.5, 1) # Set legend breaks to -1, -0.5, 0, 0.5, 1
  ) +
  theme_minimal() +
  labs(
    title = "Skill Probabilities vs Mean Reaction Time Across Levels and Encounters",
    x = "Encounter",
    y = "Skill"
  ) +
  theme(
    axis.text.x = element_text(size = 10),                  # X-axis tick labels
    axis.text.y = element_text(size = 10),                  # Y-axis tick labels
    axis.title.x = element_text(size = 14),                 # X-axis label size
    axis.title.y = element_text(size = 14),                 # Y-axis label size
    legend.title = element_text(size = 14),                 # Legend title size
    legend.key.height = unit(2, "cm"),                      # Legend key height
    plot.title = element_text(size = 14, hjust = 0.5),      # Title size and centering
    strip.text = element_text(size = 12)                    # Facet label size
  )

```
# Check whether any students have not done level 3 problems
```{r}
# Identify unique students who have solved problems in Level 1
students_level1 <- dat %>%
  filter(level == 1) %>%
  distinct(user_id) %>%
  pull(user_id)

# Identify unique students who have solved problems in Level 3
students_level3 <- dat %>%
  filter(level == 3) %>%
  distinct(user_id) %>%
  pull(user_id)

# Find students who are in Level 1 but not in Level 3
students_only_level1 <- setdiff(students_level1, students_level3)
# Find students who are in Level 3 but not in Level 1
students_only_level3 <- setdiff(students_level3, students_level1)

students_only_level1
students_only_level3

# Create dat_level1_finishers by filtering out students who only did Level 1
dat_level1_finishers <- dat_level1 %>%
  filter(!user_id %in% students_only_level1)

# Create dat_level3_finishers by filtering out students who only did Level 3
dat_level3_finishers <- dat_level3 %>%
  filter(!user_id %in% students_only_level3)

dat_level1_finishers_first <- filter_encounter(dat_level1_finishers, "first")
dat_level1_finishers_middle <- filter_encounter(dat_level1_finishers, "middle")
dat_level1_finishers_last <- filter_encounter(dat_level1_finishers, "last")

dat_level3_finishers_first <- filter_encounter(dat_level3_finishers, "first")
dat_level3_finishers_middle <- filter_encounter(dat_level3_finishers, "middle")
dat_level3_finishers_last <- filter_encounter(dat_level3_finishers, "last")
```
# Calculate and plot correlations between skill probabilties from L2 and memory decay (alpha) from L3
```{r}
# Function to calculate correlations between Level 2 skill probabilities and Level 3 memory decay rates (alpha)
calculate_alpha_correlations <- function(graaftel_data, encounter_data) {
  # Ensure cue_text alignment
  graaftel_data <- graaftel_data %>%
    mutate(cue_text = str_trim(cue_text))
  encounter_data <- encounter_data %>%
    mutate(cue_text = str_trim(cue_text))

  # Remove rows with missing cue_text
  graaftel_data <- graaftel_data %>%
    filter(!is.na(cue_text))
  encounter_data <- encounter_data %>%
    filter(!is.na(cue_text))

  # Summarise alpha (memory decay rate) by cue_text
  alpha_data <- encounter_data %>%
    group_by(cue_text) %>%
    summarise(mean_alpha = mean(alpha, na.rm = TRUE), .groups = "drop")

  # Join GraafTel data with alpha data on "cue_text"
  merged_data <- graaftel_data %>%
    left_join(alpha_data, by = "cue_text")

  # Calculate Pearson's r for each skill
  skill_correlations <- merged_data %>%
    summarise(
      skill1_r = cor(mean_alpha, skill1, use = "complete.obs"),
      skill2_r = cor(mean_alpha, skill2, use = "complete.obs"),
      skill3_r = cor(mean_alpha, skill3, use = "complete.obs")
    )

  # Reshape data for plotting
  skill_correlations_long <- skill_correlations %>%
    pivot_longer(cols = starts_with("skill"), names_to = "skill", values_to = "pearsons_r")

  return(skill_correlations_long)
}

# Combine all correlation data into a single dataframe for Level 2 skills and Level 3 memory decay rates
combine_alpha_correlation_data <- function(encounter, graaftel_data, encounter_data) {
  correlations <- calculate_alpha_correlations(graaftel_data, encounter_data)
  correlations <- correlations %>%
    mutate(level = "Level 1 to Level 3", encounter = encounter)
  return(correlations)
}

# Load GraafTel outputs for Level 1 from the /GT_ratings/finishers/ directory
finishers_s3l1first <- filter_graaftel_data("./GT_ratings/finishers13/s3l1first.csv")
finishers_s3l1middle <- filter_graaftel_data("./GT_ratings/finishers13/s3l1middle.csv")
finishers_s3l1last <- filter_graaftel_data("./GT_ratings/finishers13/s3l1last.csv")

# Create a consolidated dataframe for Level 2 skills vs Level 3 memory decay rates correlations
alpha_correlation_results <- bind_rows(
  combine_alpha_correlation_data("First", finishers_s3l1first, dat_level3_finishers_first),
  combine_alpha_correlation_data("Middle", finishers_s3l1middle, dat_level3_finishers_middle),
  combine_alpha_correlation_data("Last", finishers_s3l1last, dat_level3_finishers_last)
)

# Ensure proper ordering of encounters
alpha_correlation_results <- alpha_correlation_results %>%
  mutate(encounter = factor(encounter, levels = c("First", "Middle", "Last")))

# Replace skill labels in the dataframe
alpha_correlation_results <- alpha_correlation_results %>%
  mutate(skill = recode(skill,
                        "skill1_r" = "1",
                        "skill2_r" = "2",
                        "skill3_r" = "3"))

# Plot Pearson's r for Level 2 skills and Level 3 memory decay rates using a facet grid
ggplot(alpha_correlation_results, aes(x = encounter, y = skill, fill = pearsons_r, label = round(pearsons_r, 2))) +
  geom_tile() +
  geom_text(size = 6) +
  scale_fill_gradient2(
    limits = c(-1, 1),
    midpoint = 0,
    low = '#4575b4',
    mid = '#f7f7f7',
    high = '#d73027',
    guide = guide_colorbar(
      title = "Pearson Correlation",
      title.position = "right",
      barheight = unit(8.9, "cm")
    ),
    aesthetics = 'fill',
    breaks = c(-1, -0.5, 0, 0.5, 1),
  ) +
  theme_minimal() +
  theme(
    legend.title = element_text(angle = 90, hjust = 0.5, size = 14),
    axis.text.x = element_text(size = 8),                  # Axis tick label size (x-axis)
    axis.text.y = element_text(size = 8),                  # Axis tick label size (y-axis)
    axis.title.x = element_text(size = 14),                # X-axis label size
    axis.title.y = element_text(size = 14),                # Y-axis label size
    legend.key.height = unit(2, "cm"),                     # Legend key height
    plot.title = element_text(size = 14, hjust = 0.5),     # Title size and centering
    strip.text = element_text(size = 12)                   # Facet label size
  ) +
  labs(
    title = "Level 1 Skills vs Level 3 Memory Decay Rates (Î±) Across Encounters",
    x = "Encounter",
    y = "Skill"
  )
```
# Calculate correlations against 2s (L2)
```{r}
df_first <- s3l2first %>%
  mutate(encounter = "First")

df_middle <- s3l2middle %>%
  mutate(encounter = "Middle")

df_last <- s3l2last %>%
  mutate(encounter = "Last")

# df_all has columns: cue_text, skill1, skill2, skill3, encounter
df_all <- bind_rows(df_first, df_middle, df_last)

filter_for_number <- function(data, number) {
  # Looks for exact matches of 'number' in the cue_text (like "2" in "2 x 3")
  data %>%
    filter(str_detect(cue_text, paste0("\\b", number, "\\b")))
}

# Calculate mean skill usage by encounter
calc_means_by_encounter <- function(data) {
  data %>%
    group_by(encounter) %>%
    summarise(
      mean_skill1 = mean(skill1, na.rm = TRUE),
      mean_skill2 = mean(skill2, na.rm = TRUE),
      mean_skill3 = mean(skill3, na.rm = TRUE),
      .groups = "drop"
    )
}

compute_skill_correlations <- function(dfA, dfB, labelA, labelB) {
  # Join by `encounter` so we line up First, Middle, Last
  joined <- dfA %>%
    rename_with(~ paste0(.,"_", labelA), starts_with("mean_skill")) %>%
    inner_join(
      dfB %>% rename_with(~ paste0(.,"_", labelB), starts_with("mean_skill")),
      by = "encounter"
    )
  
  # Calculate correlation for each skill
  corr_s1 <- cor(joined[[paste0("mean_skill1_", labelA)]],
                 joined[[paste0("mean_skill1_", labelB)]],
                 use = "complete.obs")
  corr_s2 <- cor(joined[[paste0("mean_skill2_", labelA)]],
                 joined[[paste0("mean_skill2_", labelB)]],
                 use = "complete.obs")
  corr_s3 <- cor(joined[[paste0("mean_skill3_", labelA)]],
                 joined[[paste0("mean_skill3_", labelB)]],
                 use = "complete.obs")
  
  # Return a data frame with results
  tibble(
    comparison = paste0(labelA, "_vs_", labelB),
    skill = c("Skill1","Skill2","Skill3"),
    pearsons_r = c(corr_s1, corr_s2, corr_s3)
  )
}

# Generate Encounter-Means for the Groups of Interest
numbers_of_interest <- c("1","2","3","4","5","6","7","8","9","10")

# Create a named list that maps each group number to its summarized data
grouped_means <- list()
for(num in numbers_of_interest) {
  group_data <- df_all %>% 
    filter_for_number(num) %>%
    calc_means_by_encounter()
  grouped_means[[num]] <- group_data
}

# Compute Correlations for (2 vs 4), (2 vs 6), (2 vs 8), (2 vs 10) etc
corr_results <- bind_rows(
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["1"]],  "2s", "1s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["3"]],  "2s", "3s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["4"]],  "2s", "4s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["5"]],  "2s", "5s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["6"]],  "2s", "6s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["7"]],  "2s", "7s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["8"]],  "2s", "8s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["9"]],  "2s", "9s"),
  compute_skill_correlations(grouped_means[["2"]], grouped_means[["10"]], "2s", "10s")
)

# Print correlation results to console
print(corr_results)

# Heatmap Visualisation of Correlations
# Prepare data for heatmap
heatmap_data <- corr_results %>%
  mutate(
    comparison = gsub("2s_vs_", "", comparison),  # Simplify comparison labels
    skill = recode(skill, "Skill1" = "Skill 1", "Skill2" = "Skill 2", "Skill3" = "Skill 3"),
    comparison = factor(comparison, levels = c("1s", "3s", "4s", "5s", "6s", "7s", "8s", "9s", "10s"))
  )

heatmap_data <- heatmap_data %>%
  mutate(skill = as.numeric(str_extract(skill, "\\d+")))  # Extract the numeric part of 'Skill 1', 'Skill 2', etc.

# Create the heatmap
heatmap_plot <- ggplot(heatmap_data, aes(x = comparison, y = skill, fill = pearsons_r, label = round(pearsons_r, 2))) +
  geom_tile() +  # Create the heatmap tiles
  geom_text(size = 5) +  # Add Pearson's r as text in the tiles
  scale_fill_gradient2(
    low = "#4575b4",   # Blue for negative correlations
    mid = "#f7f7f7",   # White for neutral correlations
    high = "#d73027",  # Red for positive correlations
    midpoint = 0,      # Neutral midpoint
    limits = c(-1, 1), # Scale from -1 to 1
    breaks = seq(-1, 1, by = 0.5),  # Define legend breaks
    guide = guide_colorbar(
      title = "Pearson Correlation", # Define legend title
      title.position = "right",      # Position legend title
      title.theme = element_text(angle = 90, hjust = 0.5, size = 14), # Rotate and style the legend title
      barheight = unit(8.4, "cm")    # Adjust legend bar height
    )
  ) +
  theme_minimal() +
  theme(
    legend.title = element_text(angle = 90, hjust = 0.5) # Rotate and centre the legend title
  ) +
  labs(
    title = "Correlations Between 2's and Other Groups Across Skills",
    x = "Comparison Group",
    y = "Skill"
  ) +
  theme(
    axis.text.x = element_text(size = 12),                  # Axis tick label size (x-axis)
    axis.text.y = element_text(size = 12),                  # Axis tick label size (y-axis)
    axis.title.x = element_text(size = 14),                # X-axis label size
    axis.title.y = element_text(size = 14),                # Y-axis label size
    legend.key.height = unit(2, "cm"),                     # Legend key height
    plot.title = element_text(size = 16, hjust = 0.5),     # Title size and centering
    panel.grid = element_blank()                           # Remove gridlines
  )

# Print the heatmap
print(heatmap_plot)


```
# Calculate correlations against 3s (L2)
```{r}
# Compute Correlations for (3 vs 1), (3 vs 2), (3 vs 4), ..., (3 vs 10)
corr_results_3s <- bind_rows(
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["1"]],  "3s", "1s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["2"]],  "3s", "2s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["4"]],  "3s", "4s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["5"]],  "3s", "5s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["6"]],  "3s", "6s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["7"]],  "3s", "7s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["8"]],  "3s", "8s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["9"]],  "3s", "9s"),
  compute_skill_correlations(grouped_means[["3"]], grouped_means[["10"]], "3s", "10s")
)

# Print correlation results to console
print(corr_results_3s)

# Heatmap Visualisation of Correlations
# Prepare data for heatmap
heatmap_data_3s <- corr_results_3s %>%
  mutate(
    comparison = gsub("3s_vs_", "", comparison),  # Simplify comparison labels
    skill = recode(skill, "Skill1" = "Skill 1", "Skill2" = "Skill 2", "Skill3" = "Skill 3"),
    comparison = factor(comparison, levels = c("1s", "2s", "4s", "5s", "6s", "7s", "8s", "9s", "10s"))
  )

heatmap_data_3s <- heatmap_data_3s %>%
  mutate(skill = as.numeric(str_extract(skill, "\\d+")))

# Create the heatmap
heatmap_plot_3s <- ggplot(heatmap_data_3s, aes(x = comparison, y = skill, fill = pearsons_r, label = round(pearsons_r, 2))) +
  geom_tile() +  # Create the heatmap tiles
  geom_text(size = 5) +  # Add Pearson's r as text in the tiles
  scale_fill_gradient2(
    low = "#4575b4",   # Blue for negative correlations
    mid = "#f7f7f7",   # White for neutral correlations
    high = "#d73027",  # Red for positive correlations
    midpoint = 0,      # Neutral midpoint
    limits = c(-1, 1), # Scale from -1 to 1
    breaks = seq(-1, 1, by = 0.5),  # Define legend breaks
    guide = guide_colorbar(
      title = "Pearson Correlation", # Define legend title
      title.position = "right",      # Position legend title
      title.theme = element_text(angle = 90, hjust = 0.5, size = 14), # Rotate and style the legend title
      barheight = unit(8.4, "cm")    # Adjust legend bar height
    )
  ) +
  theme_minimal() +
  theme(
    legend.title = element_text(angle = 90, hjust = 0.5) # Rotate and centre the legend title
  ) +
  labs(
    title = "Correlations Between 3's and Other Groups Across Skills",
    x = "Comparison Group",
    y = "Skill"
  ) +
  theme(
    axis.text.x = element_text(size = 12),                  # Axis tick label size (x-axis)
    axis.text.y = element_text(size = 12),                  # Axis tick label size (y-axis)
    axis.title.x = element_text(size = 14),                # X-axis label size
    axis.title.y = element_text(size = 14),                # Y-axis label size
    legend.key.height = unit(2, "cm"),                     # Legend key height
    plot.title = element_text(size = 16, hjust = 0.5),     # Title size and centering
    panel.grid = element_blank()
  )

# Print the heatmap
print(heatmap_plot_3s)
```
# Calculate correlations against 1s (L2)
```{r}
# Compute Correlations for (1 vs 2), (1 vs 3), (1 vs 4), ..., (1 vs 10)
corr_results_1s <- bind_rows(
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["2"]],  "1s", "2s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["3"]],  "1s", "3s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["4"]],  "1s", "4s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["5"]],  "1s", "5s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["6"]],  "1s", "6s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["7"]],  "1s", "7s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["8"]],  "1s", "8s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["9"]],  "1s", "9s"),
  compute_skill_correlations(grouped_means[["1"]], grouped_means[["10"]], "1s", "10s")
)

# Print correlation results to console
print(corr_results_1s)

# Heatmap Visualisation of Correlations
# Prepare data for heatmap
heatmap_data_1s <- corr_results_1s %>%
  mutate(
    comparison = gsub("1s_vs_", "", comparison),  # Simplify comparison labels
    skill = recode(skill, "Skill1" = "Skill 1", "Skill2" = "Skill 2", "Skill3" = "Skill 3"),
    comparison = factor(comparison, levels = c("2s", "3s", "4s", "5s", "6s", "7s", "8s", "9s", "10s"))
  )

heatmap_data_1s <- heatmap_data_1s %>%
  mutate(skill = as.numeric(str_extract(skill, "\\d+")))  # Extract the numeric part of 'Skill 1', 'Skill 2', etc.

# Plot with updated y-axis
heatmap_plot_1s <- ggplot(heatmap_data_1s, aes(x = comparison, y = skill, fill = pearsons_r, label = round(pearsons_r, 2))) +
  geom_tile() +  # Create the heatmap tiles
  geom_text(size = 5) +  # Add Pearson's r as text in the tiles
  scale_fill_gradient2(
    low = "#4575b4",   # Blue for negative correlations
    mid = "#f7f7f7",   # White for neutral correlations
    high = "#d73027",  # Red for positive correlations
    midpoint = 0,      # Neutral midpoint
    limits = c(-1, 1), # Scale from -1 to 1
    breaks = seq(-1, 1, by = 0.5),  # Define legend breaks
    guide = guide_colorbar(
      title = "Pearson Correlation", # Define legend title
      title.position = "right",      # Position legend title
      title.theme = element_text(angle = 90, hjust = 0.5, size = 14), # Rotate and style the legend title
      barheight = unit(8.4, "cm")    # Adjust legend bar height
    )
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 12),                  # Axis tick label size (x-axis)
    axis.text.y = element_text(size = 12),                  # Axis tick label size (y-axis)
    axis.title.x = element_text(size = 14),                # X-axis label size
    axis.title.y = element_text(size = 14),                # Y-axis label size
    legend.key.height = unit(2, "cm"),                     # Legend key height
    plot.title = element_text(size = 16, hjust = 0.5),     # Title size and centering
    panel.grid = element_blank()                           # Remove gridlines
  ) +
  labs(
    title = "Correlations Between 1's and Other Groups Across Skills",
    x = "Comparison Group",
    y = "Skill"
  )

# Print the heatmap
print(heatmap_plot_1s)
```
# GT Descriptive Stats
```{r}
compute_skill_correlation <- function(encounter_list, skill_count) {
  # Combine all encounters and calculate mean for each skill
  mean_skills <- map(encounter_list, ~ select(., num_range("skill", 1:skill_count))) %>%
    reduce(`+`) / length(encounter_list)
  
  # Compute correlation matrix
  corr_mat <- cor(mean_skills, use = "complete.obs")
  
  # Rename rows and columns of the correlation matrix
  rownames(corr_mat) <- 1:skill_count
  colnames(corr_mat) <- 1:skill_count
  
  # Create plot using ggcorrplot
  ggcorrplot(corr_mat,
             type = "lower",
             lab = TRUE,
             title = paste(skill_count, "-Skill Correlation"),
             ggtheme = theme_minimal(),
             colors = c("#4575b4", "#f7f7f7", "#d73027"),
             legend.title = "Pearson Correlation") +  # Explicit legend title
    labs(
      x = "Skill",                   
      y = "Skill"                   
    ) +
    theme(
      axis.text.x = element_text(size = 12, angle = 0, vjust = 0.5),
      axis.text.y = element_text(size = 12),                 
      axis.title.x = element_text(size = 14),                
      axis.title.y = element_text(size = 14, angle = 90),    
      legend.title = element_text(size = 14, angle = 90, vjust = 0.5, hjust = 0.5),
      legend.text = element_text(size = 12, angle = 0),                
      legend.key.height = unit(1.5, "cm"),                   
      legend.position = "right",
      plot.title = element_text(size = 16, hjust = 0.5)      
    ) +
    guides(fill = guide_colorbar(title.position = "right",
                                 title.hjust = 0.5,
                                 barheight = unit(1.5, "cm"),
                                 frame.colour = "black",
                                 ticks.colour = "black"))
}

# Calculate mean correlations for 3-6 skills across all encounters
plot_3_overall <- compute_skill_correlation(
  list(s3l2first, s3l2middle, s3l2last), 3
)

plot_4_overall <- compute_skill_correlation(
  list(s4l2first, s4l2middle, s4l2last), 4
)

plot_5_overall <- compute_skill_correlation(
  list(s5l2first, s5l2middle, s5l2last), 5
)

plot_6_overall <- compute_skill_correlation(
  list(s6l2first, s6l2middle, s6l2last), 6
)

# Print the plots
print(plot_3_overall)
print(plot_4_overall)
print(plot_5_overall)
print(plot_6_overall)
```
